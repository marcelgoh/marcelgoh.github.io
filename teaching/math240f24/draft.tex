\input fontmac
\input mathmac

\input epsf
\widemargins
\bookheader{\tenpoint\sc marcel goh}{\tenpoint\sc math {\oldstyle 240} fall {\oldstyle 2024}}

\def\datestamp#1#2 {\llap{{\oldstyle #1}{\rm.}{\sc #2}\hskip12pt}}
\def\bar{\overline}
\def\symdiff{\mathrel{\triangle}}
\def\To{\Rightarrow}
\def\Gets{\Leftarrow}
\font\titlefont=cmbx12 at 14 pt
\def\cont#1#2{#1\hskip-2pt\leaders\hbox to 8pt{\hss.\hss}\hfil\hbox to 1em{\hss#2}}
\def\divides{\mid}
\def\percent{\mathrel{\%}}

\vglue24pt

\maketitle{MATH 240 Fall 2024}{notes by}{Marcel Goh}{}

\bigskip

\floattext 4.5 \ninebf A note on these notes.
\ninepoint After each class, this document will be updated with the new material that was
just covered. The datestamps in the left margin indicate when the notes from each day start.
Subsections labelled with a $*$ are optional. This document is heavily based on notes by
Jeremy Macdonald, but any errors are likely my own. Please email me if you find any.

\vskip60pt

\vskip \medskipamount \hbox to\hsize
{\relax \unhcopy \strutbox \cont
{{\tenssbx I. Foundations \kern 3pt}}{2}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 1.\kern .5em Set theory }{3}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 2.\kern .5em Propositional logic}{10}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 3.\kern .5em Predicate logic}{17}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 4.\kern .5em Proofs}{21}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 5.\kern .5em Functions}{28}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 6.\kern .5em Cardinality}{33}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 7.\kern .5em Relations}{37}}
\medskip
\noindent\hbox to\hsize{\relax \unhcopy \strutbox \cont
{{\tenssbx II. Number theory \kern 3pt}}{41}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 8.\kern .5em Division }{42}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 9.\kern .5em Primes }{47}}
\smallskip
\hbox to\hsize {\relax \unhcopy \strutbox \cont {\hskip10pt 10.\kern .5em Modular arithmetic }{51}}

\vfill\eject
\begingroup\headline{\hfil}\footline{\hfil}
\vglue160pt
\centerline{\titlefont I. FOUNDATIONS}
\vskip220pt
\bigskip
\begingroup\obeylines\eightssi
\hfill Die Mathematik ist in ihrer Entwickelung v\"ollig frei
\hfill und nur an die selbstredende R\"ucksicht gebunden,
\hfill dass ihre Begriffe sowohl in sich widerspruchslos sind,
\hfill als auch in festen durch Definitionen geordeneten Beziehungen
\hfill zu den vorher gebildeten,
\hfill bereits vorhandenen und bew\"ahrten Begriffen stehen.
\eightss
\smallskip
\hfill --- GEORG CANTOR, {\eightssi Grundlagen einer allgemeinen MannigfaltigkeitsÂ­lehre} (1883)
\endgroup%\obeylines
\bigskip\goodbreak
\vfill\eject

\advsect Set theory

\datestamp{29}{viii}
A {\it set} is a collection of distinct objects, called its {\it elements} or its {\it members}. If
$x$ is a member of set $A$, then we write $x\in A$, and if $x$ is not an element of $A$,
then we write $x\notin A$.
Sets can be written by listing out its elements. For example,
$$\bigl\{1,4,7,10,\sqrt{782}\bigr\}$$
and
$$\bigl\{\{1,2\}, \pi, \{4\}\bigr\}$$
are both sets (the second example shows that sets can themselves contain other sets). The order of the elements
is not important, and duplicate elements are ignored (so $\{1,2\} = \{2,1\} = \{1,1,2\}$).
The notation using $\{$ and $\}$
is useful for defining small, concrete examples, but expressing large sets can become
very cumbersome. The first way one can describe larger sets is to use the $\ldots$ symbol and the power of
suggestion. For instance, anyone faced with the notation
$$A = \{1,3,5,7,9,\ldots\}$$
can quickly guess that this set is supposed to contain all the positive odd integers. We can also use $\ldots$
to define finite sets. Most Canadians will be able to tell you that the set
$$\{\hbox{\rm Alberta}, \hbox{\rm British Columbia}, \ldots, \hbox{\rm Yukon}\}$$
of provinces and territories contains $13$ elements.
But this notation inherently produces some ambiguity. For example, since the sequence of positive palindromic
binary numbers starts $1,3,5,7,9,15,17,21,27,\ldots$,
we are left with some doubt as to whether the set $A$ above should be the set of odd numbers or the set of
palindromic binary numbers.

But there is another, less ambiguous way to define large sets. It is called set-builder notation
and it refers to any construction of the form
$$\bigl\{x \in U : P(x)\bigr\},$$
where $x$ is a variable, $U$ is a set,
and $P$ is a statement about $x$. The resulting set contains {\sl all $x$ such that $P(x)$
holds}. For example, letting $\NN$ denote the set $\{0,1,2,3,\ldots\}$ of counting numbers (more on this later),
to define the set of all odd numbers, we can write
$$A = \{ x\in \NN : \hbox{there exists}\ k\in \NN\ \hbox{such that}\ x = 2k + 1\}.$$
Note that the statement $P(x)$ must contain $x$, but it may also contain other previously defined symbols, as well
as new symbols defined within the statement (such as $k$ in the example above).

\medskip\boldlabel Special sets of numbers.
There are certain infinite sets of numbers that are used so often as to be given special bold notation.
Back in elementary school, you learned to use the counting numbers $0, 1, 2, 3, \ldots$. We already saw this
set in the previous paragraph; in the business,
this set is known as the {\it natural numbers}, because if you go on a nature hike you
can use them to count the number of bluebells, donkeys, etc.~that you see.
(Many mathematicians use the symbol $\NN$ to denote this set without zero. When in doubt,
clarify with the person you're talking to; in this class, $0\in \NN$.)

Sometime towards the start of junior high you were introduced to the concept of natural numbers. The set
$$\ZZ=\{ \ldots, -3, -2, -1, 0, 1, 2, 3, \ldots\}$$
is called the set of {\it integers} or {\it whole numbers}.
(The word {\it integer} just means ``whole'' in Latin; cf.~French {\it entier}. We use the letter $\ZZ$
because of the German word {\it Zahl} meaning ``number.'')

Even before you learned about negative numbers, you probably learned about fractions. They can be defined
in set-builder notation as a collection of ratios of integers, where the denominator is not zero:
$$\QQ = \{ p/q : p, q\in \ZZ, q\ne 0\}.$$
(The nitpicky reader will notice that $p/q$ is not stipulated to be a member of any set here. This is because
a rigorous definition of $\QQ$ involves quotienting out by an equivalence relation, which we don't know how to
do yet.)
This is the set of {\it rational numbers}.
Remember that sets only contain distinct elements, so $2/3$, $4/6$, $6/9$, etc.~are all considered the
same rational number.

Lastly, we have the set $\RR$ of real numbers. Constructing this set using only notions from set theory
and logic is quite the byzantine task and well outside the scope of this course, but you can think of
$\RR$ as the set of decimal numbers with a finite number of digits to the left of the decimal point, and
a possibly infinite number of digits to the right of the decimal point.

A set of numbers near and dear to many mathematicians' hearts is the set of {\it prime} numbers $P$,
defined by
$$P = \bigl\{ p\in \NN : p\ge 2,\ \hbox{and if $p = ab$ then $\{a,b\} = \{1,p\}$}\bigr\}.$$
(You may have met a different definition of prime numbers in the past. Pause a moment and convince
yourself that the statement above defines the set of prime numbers as you know it.)
\endgroup%\headline{\hfil}\footline{\hfil}

\medskip\boldlabel Set inclusion.
When we use set builder notation $B = \bigl\{x\in A : P(x)\bigr\}$, every element of $B$ is necessarily
a member of the set $A$ as well, since $B$ is defined to be the set of all $x$ in $A$ satisfying
$P(x)$. This is one way in which we can obtain
a {\it subset} of another set. More generally, we write $B\subseteq A$ if every element of $B$ is also
an element of $A$, and $B\supseteq A$ if every element of $A$ is an element of $B$. Sometimes, if $B\supseteq A$,
we say that $B$ {\it contains} $A$, or $B$ {\it includes} $A$. For example, we have the chain
$$\NN\subseteq \ZZ \subseteq \QQ\subseteq \RR$$
for the special sets of numbers defined earlier.

Symbols like $\subseteq$, $\supseteq$, and $=$ (that are
used to produce statements) can be
negated with a slash; for example, if there is some element of $B$ that is not an element of $A$,
then we write $B\not\subseteq A$.

The concept of set inclusion is important, because
the most common way to prove that two sets $A$ and $B$ are equal is to show that $A$ is a subset of $B$,
then show that $B$ is a subset of $A$. We illustrate this with the following example.

\proclaim Proposition \advthm. The sets
$$A = \{ x\in \ZZ : \hbox{there exists}\ k\in \ZZ\ \hbox{such that}\ x = 2k + 1\}$$
and
$$B = \{ x\in \ZZ : \hbox{there exists}\ l\in \ZZ\ \hbox{such that}\ x = 2l + 5\}$$
are equal. (Both are different ways of expressing the set of all odd integers.)

\proof Let $x\in A$. Then there exists $k\in\ZZ$ such that $x = 2k+1$. Letting $l = k-2$, we find that
$l$ is an integer (since $k$ was). Furthermore,
$$x = 2k+1 = 2k - 4 + 5 = 2(k-2)+5 = 2l+5.$$
We have found $l$ such that $x = 2l+5$, so $x\in B$. This shows that $A\subseteq B$.

On the other hand, let $x\in B$, so that there exists $l\in \ZZ$ such that $x = 2l+5$. Now we let $k = l+2$;
$k\in \ZZ$ since $l\in \ZZ$. We have
$$x = 2l+5 = 2l + 4 + 1 = 2(l+2) + 1 = 2k+1.$$
This shows that $x\in A$, and we have proved that $B\subseteq A$. This combined with the previous paragraph
shows that $A = B$.\slug

The above result is not so important, but pay attention to the structure of this proof. It is called a ``proof
by double inclusion,'' since we have shown that $A$ includes $B$ and $B$ includes $A$.

\medskip\boldlabel Set operations.
Now we describe a number of operations that may be performed on sets to produce other sets. They can all be
built up from the following two operations.
\medskip
\item{$\bullet$} The {\it union} $A\cup B$ of two sets $A$ and $B$ is the set of all elements that are either in
$A$ or in $B$ (or both).
\smallskip
\item{$\bullet$} The {\it intersection} $A\cap B$ of $A$ and $B$ is the set of all elements that are in both $A$
and $B$.
\medskip
As an example, if $A = \{1,2,4\}$ and $B = \{1,3,5\}$, then $A\cup B = \{1,2,3,4,5\}$ and $A\cap B = \{1\}$.

To avoid logical difficulties, we always assume that the sets we're working with are a subset of some
larger ambient set $U$, often called the {\it universe}.
Once we know what $U$ is, we may define the {\it complement}
of a set $A$ to be the set $\bar A$ of all the elements in $U$ except those that are in $A$.
So if $U = \{1,2,3,4,5\}$ in the example above, then $\bar A = \{3,5\}$ and $\bar B = \{2,4\}$.
What about $\bar{A\cup B}$? Well since $A\cup B$ is all of $U$, its complement must be empty, and we can denote
it $\{\}$. This is one valid notation for the {\it empty set}. The other is $\emptyset$.

Now is a good time to define the cardinality $|A|$ of a set $A$.
This is the number of elements in it, so $|A| = |B| = 3$
in our example, and $|A\cup B| = 5$, etc. We have $|\emptyset| = 0$, and it is possible for the cardinality of
a set to be infinity; for example, $|\NN| = \infty$. We also have $|\RR| = \infty$, but this infinity
is, in some sense, larger than $|\NN|$. (More on that later.)

Next, we define the {\it difference} $B\setminus A$ (sometimes $B-A$) of two sets.
This is the set of all elements
in $B$ that are {\it not} in $A$. So, using the complement notation we just learned about,
we can express $B\setminus A = B \cap \bar A$. It is not necessary that $A$ be a subset of $B$. In
the small example above, we have $A\setminus B = \{2,4\}$ and $B\setminus A = \{3,5\}$.

Lastly, we define the {\it symmetric difference} $A \symdiff B$ of two sets $A$ and $B$ to be the set of all
elements that are either in $A$ or in $B$ {\it but not both}.
Invoking the above example one last time, we
have $A\symdiff B = \{2,3,4,5\}$.
We can express it as using unions, intersections,
and complements as
\edef\eqsymdiffdef{\the\eqcount}
$$A \symdiff B = (A\cup B) \cap \bar{A \cap B}.\adveq$$
To practise using all the different operations we just learned,
convince yourself that the following are three more valid ways to express the symmetric difference:
$$A\symdiff B = (A\cup B)\setminus (A\cap B) = (A\setminus B) \cup (B\setminus A) =
\bar{\bar{A\cup B} \cup (A\cap B)}$$
More set identities abound. We state the following proposition without proof; you should try going
though this list and convincing yourself that each identity holds, for all sets $A$, $B$, and $C$.
(This is a great way of practising proofs by double inclusion.)

\edef\propsetlaws{\the\thmcount}
\proclaim Proposition \advthm. Let $A$, $B$, and $C$ be subsets of a universe $U$. Then
\medskip
\item{i)} $A\cap U = A$ and $A\cup \emptyset = A$;
\smallskip
\item{ii)} $A\cup U = U$ and $A\cap \emptyset = \emptyset$;
\smallskip
\item{iii)} $A\cup A = A$ and $A\cap A = A$;
\smallskip
\item{iv)} $\bar{\bar A} = A$;
\smallskip
\item{v)} $A\cup B = B\cup A$ and $A\cap B = B\cap A$;
\smallskip
\item{vi)} $A\cup (B\cup C) = (A\cup B) \cup C$ and $A\cap (B\cap C) = (A\cap B)\cap C)$;
\smallskip
\item{vii)} $A\cup (B\cap C) = (A\cup B) \cap (A\cup C)$ and $A\cap (B\cup C) = (A\cap B)\cup (A\cap C)$;
\smallskip
\item{viii)} $A\cup (A\cap B) = A$ and $A\cap (A\cup B) = A$; and
\smallskip
\item{ix)} $A\cup \bar A = U$ and $A\cap \bar A = \emptyset$.\slug
\medskip

These laws have names, some of which we will use more often than others:
(i) is called the identity law, (ii) the domination law, (iii) the idempotent law,
(iv) the law of double negation, (v) the commutative law, (vi) the associative law,
(vii) the distributive law, (viii) the absorption law, and (ix) the complement law.

\medskip\boldlabel \llap{*}Analogy with addition and multiplication.
Some of these laws bear some resemblance to laws about numbers that you already know. As an exercise,
replace $\cup$ with $+$ (addition), $\cap$ with $\cdot$ (multiplication), $\bar A$ with $-A$ (negation),
$U$ with $1$, and $\emptyset$ with $0$ in all the formulas
above, and now assume that $A$, $B$, and $C$ are arbitrary real numbers. Which identities still hold in
the number setting, and which ones don't? As a more advanced exercise, try replacing $\cup$
with $\symdiff$ in the identities above (some statements will have to be tweaked a bit so that they're
actually true, some won't). Now do the same
replacement as before, except replace $\symdiff$ with $+$. You will find that many more identities
carry over.

\medskip\boldlabel\datestamp{03}{ix} De Morgan's laws.
There are two important laws relating complements with union and intersection.
We shall state them as a proposition, this time giving a proof (of one of them).

\parenproclaim Proposition {\advthm} (De Morgan's laws). Let $A$ and $B$ be sets. Then
\medskip
\item{i)} $\bar{A\cup B} = \bar A \cap \bar B$; and
\smallskip
\item{ii)} $\bar{A\cap B} = \bar A \cup \bar B$.
\medskip

\proof
Let $x\in \bar{A\cup B}$. This means that $x$ does not belong to the union of $A$ and $B$,
$x$ cannot be in $A$, nor can it be in $B$. Since $x\notin A$, $x\in \bar A$, and since $x\notin B$,
$x\in \bar B$. Therefore, $x\in \bar A\cap \bar B$. This shows that $\bar{A\cup B}\subseteq \bar A\cap \bar B$.

Now assume that $x\in \bar A\cap \bar B$. So $x\in \bar A$ and $x\in \bar B$, meaning that $x\notin A$
and $x\notin B$. Since $x$ is in neither $A$ nor $B$, it is also not a member of the union $A\cup B$.
We conclude that $x\in \bar{A\cup B}$. We have shown that $\bar A\cap \bar B \subseteq \bar{A\cup B}$,
which fact, combined with the previous paragraph, completes the proof of (i).

The proof of (ii) is similar and left to the reader as an exercise.\slug

Armed with all of these laws, we are able to perform lots of mechanical set manipulations to simplify
expressions. For example, consider the expression
$$\bigl( (A\setminus B)\cup A\bigr) \cap \bar{A\cap \bar B}.$$
Since $A\setminus B = A\cap \bar B$ and invoking the second De Morgan law on the right of the intersection
yields
$$\bigl( (A\cap \bar B)\cup A\bigr) \cap (\bar A \cup B).$$
Now, we can use absorption on the left-hand side to obtain
$$A \cap (\bar A \cup B),$$
and then distributing gives us
$$(A\cap \bar A) \cup (A\cap B) = \emptyset \cup (A\cap B) = A\cap B.$$
We thus see that the nasty expression $\bigl( (A\setminus B)\cup A\bigr) \cap \bar{A\cap \bar B}$ is
simply another way of writing $A\cap B$.

\medskip\boldlabel The Cartesian product and power set. From the real line $\RR$, we can geometrically construct
the Cartesian plane $\RR^2$ by lining up parallel copies of $\RR$, one for each element of the original line
and all parallel to the original line.
Notationally, $\RR^2$ is the set of all ordered pairs $(a,b)$, where $a,b\in \RR$. Generalising this,
for any sets $A$ and $B$ we can define the {\it Cartesian product} $A\times B$
to be the set
$$A \times B = \bigl\{ (a,b) : a\in A, b\in B\bigr\}.$$
We sometimes write $A^2$ for $A\times A$, and more generally $A^n$ for the $n$-fold Cartesian product
of $A$ with itself. (This explains the notation $\RR^2$ for the Cartesian plane, and $\RR^n$ for the
$n$-dimensional vector space over $\RR$.)
Note that $A\times B$ is not equal to $B\times A$ in general.

\proclaim Proposition \advthm. If $A$ and $B$ are finite sets, then $|A\times B| = |A|\cdot |B|$.

\proof The set $A\times B$ consists of all ordered pairs $(a,b)$ where $a\in A$ and $b\in B$. There are
$|A|$ choices for $a$, and for $a$, there are $|B|$ ways to pair it with a $b$ from $B$. So there
are $|A|\cdot |B|$ pairs in total.\slug

Now we define the {\it power set}. For a set $A$, this is the set of all subsets of $A$, and is
commonly denoted by ${\cal P}(A)$ or $2^A$. (We will use the latter notation in these notes.)
In set-builder notation, we have
$$2^A = \{X\subseteq U : X\subseteq A\}.$$
As an example, if $A = \{1,2,3\}$, then
$$2^A = \bigl\{\emptyset, \{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1,2,3\}\bigr\}.$$
Note that even though, say, $1\in A$, we do not have $1\in 2^A$. We do, however, have $\{1\}\in 2^A$.

Another example is $2^\ZZ$, the set of all subsets of integers. If $P$ is the set of primes, then
$P\in 2^\ZZ$. Also in $2^\ZZ$ is the set $S = \{n^2 : n\in \ZZ\}$ of square numbers.

There is a way of encoding subsets with strings of $0$s and $1$s. Suppose we have a set
$$\{-3, 1, 7, 19, 23\}.$$
Fixing this order of the elements, for any arbitrary subset of this set, we can associate to it a binary string.
Consider the subset $\{1, 19, 23\}$. This corresponds to the binary string $01011$: since the first element,
$3$, is not in the subset, we write a $0$. Then $1$ is in the subset, so we write a $1$, and so on.
This is a reversible process. Given a binary string of length $5$, say, $00101$, we can reconstruct the
subset that corresponds to it. The first two $0$s mean that $-3$ and $1$ do not belong to the set,
but $7$ does, $19$ doesn't, and $23$ does. So the subset is $\{7, 23\}$. In this way we see that there
is a one-to-one correspondence between the elements of $2^A$ and binary strings of length $|A|$.
We'll use this fact in the proof of the following proposition.

\edef\propcardpowerset{\the\thmcount}
\proclaim Proposition \advthm. If $A$ is finite, then
$|2^A| = 2^{|A|}$.

\proof We just saw that there is a one-to-one correspondence between elements of $2^A$ and binary strings
of length $|A|$. So it suffices to count the number of binary strings of length $|A|$. Well, each digit
can be either $0$ or $1$, and there are $|A|$ digits, so the number of strings is
$$\underbrace{2\cdot 2\cdot \cdots \cdot 2}_{|A|\ \hbox{\sevenrm times}} = 2^{|A|}.\noskipslug$$

\medskip\boldlabel Counterexamples in proofs.
We finish this subsection with a little example problem. {\sl Is it true that $2^A \cup 2^B = 2^{A\cup B}$
for all sets $A$ and $B$?}

Let's start by trying to prove the statement is true. As usual, we will attempt a double-inclusion proof.
Let $2^A\cup 2^B$. This means $X$ is either a subset of $A$ or it is a subset of $B$. Either way,
$X$ is a subset of $A\cup B$, so $X\in 2^{A\cup B}$. So far so good; we have proved that
$2^A\cup 2^B \subseteq 2^{A\cup B}$.

Now we try the other direction. Let $X\in 2^{A\cup B}$. So $X$ is a subset of $A\cup B$. From here
we want to say that $X$ must be a subset of $A$ or it must be a subset of $B$, but is that necessarily true?
It is possible that $X$ is contained slightly in $A$ and slightly in $B$. So we have failed
to prove that $2^{A\cup B} \subseteq 2^A \cup 2^B$ in general. But just because we have failed to prove
that something is true doesn't mean we have proved it is false!

To actually prove that $2^{A\cup B} \subseteq 2^A \cup 2^B$ doesn't hold in general, we need to find
a {\it counterexample}. That is, we need to construct sets $A$ and $B$ such that the statement is false.
In this case, we can let $A = \{1,2\}$, $B = \{3,4\}$, so that $A\cup B = \{1,2,3,4\}$.
Then the set $\{1,3\}$ is a subset of $A\cup B$ but is not a subset of $A$ and it is not a subset of $B$.
In other words, $\{1,3\}\in 2^{A\cup B}$ but $\{1,3\} \notin 2^A\cup 2^B$, proving that
$2^{A\cup B} \not\subseteq 2^A \cup 2^B$.

\medskip\boldlabel\datestamp{05}{ix} Russell's paradox. Earlier, we said that the sets we are working with
need to be a subset of a universe $U$, which has already been proved to be a set. We gave lots of
ways to make sets out of new sets, such as the union and intersection operations, etc. Starting with
the assumption that the empty set $\emptyset$ is a set,
it is possible to define the set of natural numbers as follows.
We can define $0 = \emptyset$, $1 = \{0\}$, and $2 = \{0,1\}$, and so on.
Now we take the set of all of these, and call this $\NN$.
(We can also define addition and multiplication on these set-theoretic ``numbers''
so that they behave like addition and multiplication do on $\NN$.) From here we can do more funky stuff
to define $\ZZ$, $\QQ$, and $\RR$, and prove that these are all sets (you can see this in a higher-level
course on set theory, if you're interested). So there isn't much of a problem
with all the sets we have played with so far; they are all subsets of things that are already known to be sets.

Ungodly things can happen if we don't stick by these rules. An example, due to Bertrand Russell, is the ``set''
$$R = \{\hbox{sets}\ X : X \notin X \}.$$
In plain English, $R$ is defined to be the set of all sets that contain themselves.
This is not a subset of any known thing, so by our criterion above we would not consider it a set.
But supposing it is, let us ask ourselves the following question. Does $R$ contain itself?
If it does not, then $R\notin R$, so $R$ would be a set that satisfied the condition of $R$, so $R\in R$.
But on the other hand, if $R\in R$, then $R$ violates the condition defining $R$, so $R\notin R$.
Round and round we go in a circle of contradiction.

Such is the price of meddling with ``sets'' that aren't subsets of known sets. This also shows that there
is no such thing as ``the set of all sets.''

\advsect Propositional logic

A {\it proposition} is a statement that is true or false. For example ``$8$ is even'' is a statement
we know to be true, and ``$8$ is prime'' is a statement we know to be false. The statement
``$n$ is prime'' is not a proposition because its truth or falsity depends on what $n$ is.
The statement
``$2^{2^{240}} - 1$ is prime'' is a proposition, because it is either true or false (even though you or I
might not know which one it is).

A {\it propositional variable} or a {\it boolean variable} is a variable which can take either the
value $0$ or $1$, where $0$ means ``false'' and $1$ means ``true.'' Usually we use letters $p$, $q$,
and $r$ to denote propositional variables. The simplest logical operator is negation, defined
by the table
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil \cr
p & \neg p \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 1 \cr
1 & 0 \cr
}}.
$$
This is also called the {\mc NOT} operator, since if $p$ is true, then $\neg p$
is false, and vice versa.
Next is {\it conjunction}, which has the table
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & p\wedge q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 0 \cr
0 & 1 & 0 \cr
1 & 0 & 0 \cr
1 & 1 & 1 \cr
}}.
$$
This is also called the {\mc AND} operator, because $p\wedge q$ is true if and only if $p$ and
$q$ are both true. The {\mc OR} operator, also called {\it disjunction}, has the table
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & p\vee q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 0 \cr
0 & 1 & 1 \cr
1 & 0 & 1 \cr
1 & 1 & 1 \cr
}}.
$$
We see that $p\vee q$ is true if $p$ or $q$ is true (or both). The symbol $\vee$ is meant to recall
the Latin word {\it vel}, meaning ``or.'' (One of the most important early treatises on mathematical
logic and set theory was {\sl Arithmetices principia, nova methodo exposita}, published in Latin in 1889
by Giuseppe Peano. It established the now-standard axiomatisation of the natural numbers.)

On the other hand, in English, we often use the word ``or''
to mean an {\it exclusive} or; that is, either $p$ and $q$ are true but not both. In mathematics, on the
other hand, ``or'' is usually {\it inclusive}, so both $p$ and $q$ are allowed to hold at the same time.
It is possible to express the exclusive disjunction (often called {\mc XOR})
by a table, however. We will use the symbol $\oplus$ for this operator, and its table looks like this:
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & p\oplus q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 0 \cr
0 & 1 & 1 \cr
1 & 0 & 1 \cr
1 & 1 & 0 \cr
}}
$$
So $p\oplus q$ is true if $p$ is true or $q$ is true, but not both.

A {\it formula} is an expression containing propositional variables, $0$, $1$, logical operators, and
parentheses. The formula must syntactically make sense; for instance, $0 (\vee \wedge q \neg $ is
not a formula. Just as in ordinary mathematical notation,
parentheses are used to clarify which operators should be evaluated first. We will assume that negation
applies first, but an expression such as $p\vee q \wedge r$ is ambiguous. (In many programming languages,
conjunction has higher priority than disjunction, but in this class, just add parentheses to clarify.)

Above we have illustrated the basic logical operators
by writing out their {\it truth tables}. These are tables that give the value of
a formula for all possible values of its variables. We can write truth tables for more complex formulas as
well:
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & p\wedge q & \neg(p \wedge q) & \neg(p\wedge q) \oplus q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 0 & 1 & 1 \cr
0 & 1 & 0 & 1 & 0 \cr
1 & 0 & 0 & 1 & 1 \cr
1 & 1 & 1 & 0 & 1 \cr
}}
$$
Strictly speaking, the third and fourth columns are not necessary, but these intermediary columns help us
verify the accuracy of the following ones.

Two formulas $f_1$ and $f_2$ are said to be {\it logically equivalent} if they have the same truth table;
that is, they produce the same output if given the same input. In this case we write $f_1 \equiv f_2$.
For example, let $f_1 = \neg(p\wedge q) \oplus q$, the formula whose truth table is illustrated above.
Now let $f_2 = p \vee \neg q$. Its truth table is
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & \neg q & p\vee \neg q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 1 & 1 \cr
0 & 1 & 0 & 0 \cr
1 & 0 & 1 & 1 \cr
1 & 1 & 0 & 1 \cr
}},
$$
so we conclude that $f_1 \equiv f_2$. As a larger example, suppose we want to find all
values of $p$, $q$, and $r$ such that
$$ f = (p\vee q)\wedge (\neg q \vee \neg r)$$
evaluates to $1$. The truth table
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil &
\hfil$#$\hfil & \hfil$#$\hfil  \cr
p & q & r & p\vee q & \neg q \vee \neg r & f \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 0 & 0 & 1 & 0 \cr
0 & 0 & 1 & 0 & 1 & 0 \cr
0 & 1 & 0 & 1 & 1 & 1 \cr
0 & 1 & 1 & 1 & 0 & 0 \cr
1 & 0 & 0 & 1 & 1 & 1 \cr
1 & 0 & 1 & 1 & 1 & 1 \cr
1 & 1 & 0 & 1 & 1 & 1 \cr
1 & 1 & 1 & 1 & 0 & 0 \cr
}}
$$
shows that $f$ evaluates to $1$ precisely when
$$(p,q,r) \in \bigl\{(0,1,0), (1,0,0), (1,0,1), (1,1,0)\bigr\}.$$
To write out the truth table for a formula with $n$ variables, we need $2^n$ rows, so this
method is unsuitable for formulas with more than three or four variables.

\medskip\boldlabel Simplifying logical formulas. Just as we have rules for simplifying set expressions,
there are ways to turn complicated logical formulas into simpler ones that are logically equivalent.
What might surprise you is that the rules turn out to be exactly the same! To see the basis for this
correspondence, consider the definition of a union of sets $A$ and $B$. In set-builder notation,
this is
$$A \cup B = \{ x\in U : x\in A\ \hbox{or}\ x\in B\}.$$
The ``or'' in the definition suggests that $\cup$ is intimately related to the $\vee$ operation in
propositional logic. Repeating this process, we have the ``dictionary''
$$
\vbox{
\tabskip=3em plus.005em minus .005em
\halign{
\hfil{\ninepoint #}\hfil & \hfil{\ninepoint #}\hfil \cr
Set theory & Propositional logic\cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
sets $A$, $B$ & variables $p$, $q$ \cr
unions $A\cup B$ & disjunctions $p\vee q$ \cr
intersections $A\cap B$ & conjunctions $p\wedge q$ \cr
complements $\bar A$ & negations $\neg p$ \cr
symmetric differences $A\symdiff B$ & exclusive disjunctions $p\oplus q$ \cr
the empty set $\emptyset$ & $0$ \cr
the universe $U$ & $1$ \cr
}}
$$
Exploiting this connection, we have the following analogue of Proposition~{\propsetlaws}.

\proclaim Proposition 2. Let $p$, $q$, and $r$ be propositional variables. Then
\medskip
\item{i)} $p\wedge 1 \equiv p$ and $p\vee 0 \equiv p$;
\smallskip
\item{ii)} $p \vee 1 \equiv 1$ and $p \wedge 0 \equiv 0$;
\smallskip
\item{iii)} $p \vee p \equiv p$ and $p\wedge p \equiv p$;
\smallskip
\item{iv)} $\neg\neg p \equiv p$;
\smallskip
\item{v)} $p \vee q \equiv q \vee p$ and $p\wedge q \equiv q\wedge p $;
\smallskip
\item{vi)} $p\vee (q\vee r) \equiv (p\vee q) \vee r$ and $p\wedge (q\wedge r) \equiv (p\wedge q)\wedge r)$;
\smallskip
\item{vii)} $p\vee (q\wedge r) \equiv (p\vee q) \wedge (p\vee r)$ and
$p\wedge (q\vee r) \equiv (p\wedge q)\vee (p\wedge r)$;
\smallskip
\item{viii)} $p\vee (p\wedge q) \equiv p$ and $p\wedge (p\vee q) \equiv p$; and
\smallskip
\item{ix)} $p\vee \bar p \equiv 1$ and $p\wedge \bar p \equiv 0$.\slug
\medskip

Since they are essentially the same as their set equivalents, the names of these laws are the same as in the
realm of sets.
We also have the propositional equivalent of De Morgan's law, which states that
\medskip\begingroup\sl
\item{x)} $\neg(p\vee q) \equiv \neg p \wedge \neg q$ and $\neg(p\wedge q) \equiv \neg p \vee \neg q$.\endgroup%sl
\medskip

Using these rules, we can now show that $\neg(p\wedge q) \oplus q \equiv p\vee\neg q$, which we
already saw earlier from their truth tables. First we observe that
\edef\eqxordef{\the\eqcount}
$$p\oplus q \equiv (p\vee q)\wedge \neg(p\wedge q),\adveq$$
which is analogous to the identity~\refeq{\eqsymdiffdef} for symmetric differences. So
$$\eqalign{
\neg(p\wedge q)\oplus q&\equiv\bigl(\neg(p\wedge q)\vee q\bigr)\wedge\neg\bigl(\neg(p\wedge q)\wedge q\bigr)\cr
&\equiv (\neg p \vee\neg q \vee q) \wedge \bigl((p\wedge q) \vee \neg q\bigr) \cr
&\equiv (\neg p \vee 1) \wedge \bigl((p\vee \neg q) \wedge (q\vee \neg q)\bigr) \cr
&\equiv 1 \wedge \bigl((p\vee \neg q) \wedge 1\bigr) \cr
&\equiv p\vee \neg q, \cr
}$$
where in the first line we use~\refeq{\eqxordef}, in the second line we use De Morgan's law twice, in
the third line we use the complement and distributive laws, the fourth line we use the domination and
complement laws, and in the last line we use the identity law (twice). (It is not super important
to remember the names of all these laws, as long as you remember their statements, but you may actually find
it is easier to remember the names along with the statements, just as it might be easier to remember faces of
people you've met if you also know their names.)

\medskip\boldlabel\datestamp{10}{ix} Conditional and biconditional. We now examine the {\it conditional}
logical relation {\mc IF} $p$ {\mc THEN} $q$. It is denoted by $p\To q$ and its truth table is given by
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & p\To q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 1 \cr
0 & 1 & 1 \cr
1 & 0 & 0 \cr
1 & 1 & 1 \cr
}}.
$$
Within the conditional statement, $p$ is called the {\it antecedent}; this is the assumption. The
statement that is asserted, conditional that the antecendent holds, is called the {\it consequent} $q$.
You can quickly check that the relation $p\To q$ is equivalent to $\neg p \vee q$. This fact is useful
when performing mechanical simplifications.
In English, the statement ``if $p$ then $q$'' asserts a causal relation between $p$ and $q$.
Take a moment and reconcile this idea with the truth table above.
It is normal to get a little tripped up if it's your first time seeing this table.
In the first row, $q$ doesn't even happen, so it might feel weird to say that $p$
has ``caused'' $q$ to happen in this case, and
in the second row, $p$ is not true
and $q$ is true, so it might seem odd that we have set $p\To q$ to true, because there seems to be no relation
between $p$ and $q$.
But it should make sense if you think
of $p$ as a precondition to a promise $q$, and then considering whether the promise is broken. As an example,
suppose your friend says, If it snows tomorrow I'll work in Trottier with you. If it doesn't snow
and she doesn't pull up, she hasn't technically broken her promise. If it doesn't
snow and she shows up, then she still hasn't broken her promise. The only way she can break her promise is if
it snows and she doesn't come to Trottier; this situation corresponds to the only $0$-row in the truth table.

The conditional is a very important logical operator to understand, because most theorem statements assume
some hypothesis and claim some conclusion. You will be asked to prove statements of this form, so it is
important to understand the logical nature of the statements to begin with.

The last relation is called the {\it biconditional}, and it asserts that variables $p$ and $q$ are logically
equivalent. That is, $p$ happens if and only if $q$ happens. It's truth table
$$
\vbox{
\tabskip=1em plus.2em minus .2em
\halign{
\hfil$#$\hfil & \hfil$#$\hfil & \hfil$#$\hfil \cr
p & q & p\Leftrightarrow q \cr
\noalign{\medskip}
\noalign{\hrule}
\noalign{\medskip}
0 & 0 & 1 \cr
0 & 1 & 0 \cr
1 & 0 & 0 \cr
1 & 1 & 1 \cr
}}
$$
is pretty self-explanatory; in it, $p\Leftrightarrow q$ is true whenever $p$ and $q$ have the same truth value.
The name ``biconditional'' is suggested by (part of) the following proposition.

\proclaim Proposition {\advthm}. Let $p$ and $q$ be propositional variables. Then
$$\eqalign{
p \Leftrightarrow q &\equiv (p\To q) \wedge (q\To p) \cr
&\equiv \bigl( (\neg p)\vee q\bigr) \wedge \bigl( (\neg q)\vee p\bigr) \cr
&\equiv (p\wedge q) \vee \bigl( (\neg p) \wedge (\neg q)\bigr).\cr
}$$

\proof We leave the first equivalence to the reader (writing out the truth table is one way of proving it).
The second equivalence follows from our earlier observation that $p\To q \equiv \neg p \vee q$, and
the third equivalence follows from the distributive, complement, and identity laws.\slug

A formula $f$ is called a
\medskip
\item{i)} {\it tautology} if $f\equiv 1$, i.e., $f$ always evaluates to true;
\smallskip
\item{ii)} {\it contradiction} if $f\equiv 0$, i.e., $f$ always evaluates to false;
\smallskip
\item{iii)} {\it contingency} if $f$ can evaluate to both $1$ and $0$, depending on the
values of its variables;
\smallskip
\item{iv)} {\it satisfiable} if $f$ evaluates to $1$ for at least one input; and
\smallskip
\item{v)} {\it falsifiable} if $f$ evaluates to $0$ for at least one input.
\medskip
An example of a tautology is $p\vee\neg p$ and an example of a contradiction is $p\wedge \neg p$.
This follows from the complement laws. To say that something is satisfiable is precisely to
say that it is not a contradiction, and to say that something is falsifiable is equivalent to saying
that it is not a tautology. Contingencies are those formulas that are both satisfiable
and falsifiable (in other words, formulas that are neither tautologies nor contradictions).

Suppose we are asked which of the above definitions the formula
$$f \equiv \bigl( p\wedge (p\To q)\bigr) \To q$$
satisfies. This formula only has two variables, so it is easy enough to use a truth table
for this purpose, but we will take the opportunity to practise simplifying the expression
symbolically. First of all, let's change all conditionals of the form $r \To s$,
to disjunctions of the form $\neg r \vee s$. This gives us
$$f\equiv \neg\bigl( p\wedge (\neg p \vee q)\bigr) \vee q.$$
Now we use the distributive law to distribute the conjunction over the innermost disjunction,
obtaining
$$f\equiv \neg\bigl((p\wedge \neg p) \vee (p\wedge q)\bigr) \vee q;$$
by the complement and identity laws in that order, this simplifies to
$$f\equiv \neg(p\wedge q) \vee q.$$
Now De Morgan's law and associativity give
$$f\equiv (\neg p \vee \neg q) \vee q \equiv \neg p \vee (\neg q \vee q),$$
and thus
$$f\equiv \neg p \vee 1 \equiv 1,$$
by the complement and domination laws in that order. We conclude that $f$ is a tautology,
which also means that it is satisfiable.

The fact that $\bigl( p\wedge (p\To q)\bigr) \To q$ is a tautology symbolically justifies
the argument that of $p$ is true and $p\To q$ is true, then we should be able to conclude $q$.
This form of argument is called {\it modus ponens}, and it dates back to ancient times. You probably
use {\it modus ponens} all the time in everyday life without knowing it, and we will
certainly use it in this class a lot.

\medskip\boldlabel Encoding problems in propositional logic.
Many algorithmic and logical problems can be encoded in propositional logic (and then later
solved by a computer program). For example, suppose we want to play $4\times 4$ Sudoku. In this
game, we have a $4\times 4$ grid and we want to fill it with the numbers $1$ through $4$ such that
\medskip
\item{i)} every row contains $1$ through $4$;
\smallskip
\item{ii)} every column contains $1$ through $4$; and
\smallskip
\item{iii)} the four subsquares each contain $1$ through $4$.
\medskip
In a given instance of the game, some cells are already filled in. The puzzle is: {\sl Is there a
solution and if so, what is it?}
\midinsert
$$\epsfbox{sudoku.ps}$$
\vskip5pt
\caption{An example $4\times 4$ Sudoku game.}
\endinsert
\goodbreak
To represent a Sudoku game in propositional logic, we can define boolean variables
$p_{i,j,k}$, where $i$, $j$, and $k$ range over $\{1,2,3,4\}$. (So there are $4^3$ variables
in total.) We shall set
$$ p_{i,j,k} = \cases{1, & if the number $k$ is in row $i$ and column $j$;\cr 0, & otherwise.}$$
We'll  number the rows increasing from the top and the columns increasing
from left to right.
For example, in Fig.~1 there is a $2$ in row $1$ and column $3$, so $p_{1,3,2} = 1$.

Now we set to work encoding the conditions of a Sudoku grid in propositional logic:
\medskip
\item{i)} To stipulate that every row contain $1$ through $4$, we first define auxiliary variables
$$r_{i,k} = p_{i,1,k} \vee p_{i,2,k} \vee p_{i,3,k} \vee p_{i,3,k},$$
for $i,k\in \{1,2,3,4\}$.
With these helper variables, we now see that
$$ r_{1,1} \wedge r_{1,2} \wedge r_{1,3} \wedge r_{1,4} $$
encodes the requirement that row $1$ contains one of each number.
We do the same for rows $2$, $3$, and $4$ as well, and then combine with {\mc AND}.
\smallskip
\item{ii)} We do a similar thing as in part (i) for each of the four columns.
\smallskip
\item{iii)} Ditto for subsquares.
\smallskip
\item{iv)} We need to set the initial values of the grid. For the grid in Fig.~1, we have
the formula
$$p_{1,3,2} \wedge p_{3,1,1} \wedge p_{3,2,3}.$$
\smallskip
\item{v)} Lastly, we need to make sure that there is not more than one number per cell. To do
this, for each cell $(i,j)\in \{1,2,3,4\}^2$ we write
$$ p_{i,j,1} \To (\neg p_{i,j,2} \wedge \neg p_{i,j,3} \wedge \neg p_{i,j,4}),$$
and so on (four conditionals in total). Of course we'll need to {\mc AND} all these together.
\medskip
Now we combine the formulas from each of these five steps into one long formula $f$ such that
$f$ is satisfiable if and only if the grid has a solution, and the values for $p_{i,j,k}$ give
a solution.

Defining all of these variables was a rather arduous and cumbersome process, and not entirely worth
it for a $4\times 4$ game of Sudoku (which can just be solved by eyeballing the grid).
But one could imagine writing a general computer program to encode larger and larger grids.
In fact, there are lots of problems that can be {\it reduced} to the problem of
determining if a boolean formula is satisfiable. This means that if we have a program
capable of taking a formula $f$ as input and spitting out whether or not it is satisfiable (in a
reasonable amount of time), then there are lots of real-world problems that this program
could be applied to.

This problem is called the {\it boolean satisfiability problem}, often abbreviated {\mc SAT}. One
way of solving it for any given $f$ is to just compute its truth table. We already know the
downside of this approach: if $f$ has $n$ variables, then its truth table will have $2^n$ rows.
Given a few minutes, you are certainly capable of writing down a formula $f$ that has $300$
variables, call them $p_1, \ldots, p_{300}$. The truth table of $f$ will have $2^{300}$ rows,
which is more than the number of atoms in the observable universe. You can learn a lot more
about {\mc SAT} in a higher-level class on computational complexity (e.g., {\mc COMP 360/362}).

\advsect Predicate logic

\datestamp{12}{ix}
Propositional logic allows us to work with simple declarations, but this isn't powerful enough
to express some deeper mathematical concepts. For this purpose, we now introduce the notion of
a {\it predicate}. This is a statement involving some number of variables, each of which may take
values coming from a universe $U$, such that the statement evaluates to either true or false {\sl once
all variables are assigned values}. The statement $P(n)$ given by ``$n$ is prime''
is an example of this, where $n$
can take any value in the universe $\ZZ$. An example with two variables is the
predicate $L(x,y)$ defined by ``$x$ is less than $y$.''
(A more commonly-used notation for this predicate is ``$x<y$.'')

Predicates contain variables, but at the moment we don't have any way of introducing new variables into
a statement. This is done using two different {\it quantifiers}.
The first is the
{\it universal quantifier}, denoted $\forall$ and meaning ``for all.'' The statement $\forall n:P(n)$
is true if and only if $P(n)$ is true for every possible value that $n$ can take. The second quantifier
is the {\it existential quantifier}, written ``$\exists$'' and with the meaning ``there exists.''
The statement $\exists n:P(n)$ is true if and only if there is (at least) one value that $n$ can take
such that $P(n)$ is true. The colon doesn't really have any mathematical meaning in these formulas;
they just visually set the quantifiers apart from the predicates that follow.

For instance, taking $P(n)$ to be the statement ``$n$ is prime,'' where the universe $U$ is $\NN$,
the statement $\exists n P(n)$ is true and the statement $\forall n:P(n)$ is false.
What about the statement $\forall x\,\exists y:y< x$? Well, if the universe $U$ is taken to be $\NN$,
then the statement is false, because setting $x$ equal to $0$, there is no element $y$ of $\NN$ such that
$y<0$. But if $U = \ZZ$, then the statement is true, since for every integer $x$, we can put $y = x-1$,
so that $y<x$.

Now we practise translating converting mathematical statements written in English into formulas
in predicate logic. Suppose we want to write, Every integer is even or odd. The universe here is
is the set $\ZZ$ of integers. The word ``every'' has the same meaning as ``for all,'' so right
off the bat, we can reexpress the statement as, For all $n\in \ZZ$, $n$ is even or $n$ is odd.
In symbols, this is
$$ \forall n\,(n\ \hbox{even} \vee n\ \hbox{odd}).$$
Lastly, we need to figure out how to express the property of being even or being odd.
An integer $n$ is even if an only if it is a multiple of two; that is, if there is some integer $k$
such that $2k = n$. Likewise, an integer is odd if and only if it is one more than a multiple of two.
The corresponding formula is $\exists k : 2k+1 = n$.
So our statement can be expressed
$$\forall n\, \bigl((\exists k : n = 2k)\vee (\exists k : n = 2k+1)\bigr).$$
The variable $k$ appears twice in this formula, but its first instance is independent of its second instance,
because of the parentheses. (Readers who write computer programs will be familiar with the concept of
a variable ``going out of scope.'') So there is nothing wrong with this formula, but to be extra
clear that the first $k$ is different from the second $k$, why don't we replace it with a different letter?
Thus we arrive at
$$\forall n\, \bigl((\exists k : n = 2k)\vee (\exists l : n = 2l+1)\bigr),$$
a formula in predicate logic that means, Every integer is even or odd.

\medskip\boldlabel Restrictions using quantifiers. It is not true that every real number has a multiplicative
inverse, since one cannot divide by zero. However, the statement ``every nonzero real number has a multiplicative
inverse'' is true. How should we write this as a formula in predicate logic?
One way is to use the conditional: over the universe $U = \RR$, we could write
$$\forall x : (x\ne 0 \To \exists y : xy = 1).$$
Another is to use subscripts: in the same universe, we write
$$\forall x_{x\ne 0}\, \exists y : xy = 1.$$
Using subscripts is slightly informal, since we didn't formally define above what a subscript is supposed
to mean, but it is something that you might encounter. The last way is to simply restrict the universe itself:
in the universe $U = \RR\setminus\{0\}$, the statement
$$\forall x\, \exists y : xy = 1$$
is true.

\medskip\boldlabel Multiple quantifiers. Withing a formula, quantifiers cannot be interchanged
willy-nilly. The order of $\forall$ and $\exists$ matters! They are read from left to right.
Consider the following examples, over the universe $U = \RR$.
The statement
$$ \forall x\, \exists y : x+y = 0$$
is true, since for each given $x$ we can take $y$ to be $-x$. On the other hand,
$$ \exists y\, \forall x : x+y = 0$$
is false, since it would mean that there is some integer that adds up to zero with {\it any} integer.
Of course, sometimes switching the order of quantifiers, doesn't change the truth value of a statement.
Both
$$\exists y\, \forall x: xy = 0$$
and
$$\forall x\, \exists y: xy = 0$$
are true, since in the first case, we can take $y = 0$, and in the second case, we can set $y$ to $0$ no
matter what $x$ is given.

So we know that the order of $\forall$ and $\exists$ matters in general, but repeated instances
of the {\it same} quantifier {\it can} be interchanged. For instance,
$$\forall x\, \forall y : x^2 + y^4 \ge 0$$
is the same as
$$\forall y\, \forall x : x^2 + y^4 \ge 0,$$
and we can even write $\forall x,y : x^2 + y^4 \ge 0$, to introduce both variables simultaneously.

\medskip\boldlabel Negating quantifiers. The universal quantifier is sort of like a big chain of conjunctions
that goes over the whole of the universe. For example, in the universe $\NN$, the
statement $\forall n : P(n)$ is equivalent to
$$P(1) \wedge P(2) \wedge P(3) \wedge \cdots,$$
if this were a valid propositional formula (it isn't because we don't allow propositional formulas to be
infinite). Likewise, the existential quantifier $\exists n : P(n)$ is equivalent to
$$P(1)\vee P(2) \vee P(3) \vee\cdots.$$
We know, by De Morgan's laws, that negating a big series of conjunctions requires us to
flip all the {\mc AND}s to {\mc OR}s. So, once again abusing notation somewhat, we expect
$$\neg\bigl(P(1) \wedge P(2) \wedge P(3) \wedge \cdots\bigr)
\equiv \neg P(1) \vee \neg P(2)\vee \neg P(3)\vee\cdots.$$
Thus we conclude that
$$\neg\bigl(\forall n : P(n)\bigr) \equiv \exists n : \neg P(n).$$
You can play the same game with the other De Morgan's law to show that
$$\neg \bigl(\exists n : P(n)\bigr) \equiv \forall n : \neg P(n).$$
Going back to our example of $P(n)$ denoting ``$n$ is prime,'' the statement $\neg \bigl( \forall n : P(n)\bigr)$
is true, since not all integers $n$ are prime, and we have just shown that this is equivalent to the
statement $\exists n : \neg P(n)$; that is, there exists $n$ such that $n$ is not prime.

We end this section with a longer example. Let's express the statement, ``There is a nonzero
real number such that every real number is not its inverse or is negative.'' In the universe $\RR$,
the formula corresponding to this statement is
$$ \exists x : ( x\ne 0 \wedge (\forall y : xy\ne 1 \vee y< 0)\bigr) .$$
(Work it out yourself!) Is this statement true or false? It turns out that it is true. You might be able
to stare at the formula long enough to convince yourself of this fact, but another way to see
that it's true is to note that its negation is false. Let's do this now (it's a good excuse to
practise negating a formula). We have
$$\eqalign{
\neg\Bigl(\exists x :( x\ne 0 \wedge (\forall y : x&y\ne 1 \vee y< 0)\bigr)\Bigr) \cr
&\equiv \forall x : \neg ( x\ne 0 \wedge (\forall y : xy\ne 1 \vee y< 0)\bigr) \cr
&\equiv \forall x : \bigr(x= 0 \vee \neg(\forall y : xy\ne 1 \vee y< 0)\bigr) \cr
&\equiv \forall x : \bigr(x= 0 \vee \exists y : \neg (xy\ne 1 \vee y< 0)\bigr) \cr
&\equiv \forall x : \bigr(x= 0 \vee \exists y : (xy = 1 \wedge y\ge 0)\bigr).\cr
}$$
This negated statement is false, since if $x = -2$, then $x=0$ doesn't hold, so the left-hand
side of the {\mc OR} isn't true, and there is no $y$ such that $-2 y = 1$ and $y\ge 0$ are both
true, since the only $y$ satisfying $-2y = 1$ is $-1/2$.

Negating a formula in predicate logic is entirely mechanical.
The $\neg$ symbol moves from left to right like a bulldozer that flips quantifiers and negates
predicates it finds along the way, until eventually its job is done and it disappears.
More broadly, we write mathematical statements in formal logic to make things more precise and mechanical.
This can be useful to humans, since English is often ambiguous whereas the notation we just established
is not. It can be useful to machines as well,
since, as we just saw, manipulating a formula is something that can very easily be automated.

\advsect Proofs

We're getting to the fun part of the course now.
Further back in these notes, we already proved a few statements about sets. This was just a taste of
what's to come, as the main focus of this course is to teach you how to prove mathematical statements.
These will all be statements that can ultimately be stated in predicate logic, so using the tools
from the previous section, you can boil them down to their logical skeleton. In this section, we will
formally define what it means to prove a statement in predicate logic.

To prove a statement, we always process the quantifiers from left to right. Whenever we encounter something
of the form $\exists x: P(x)$, we are allowed to choose the value for $x$ (from the given universe), and we
just need to show that $P(x)$ holds for that value of $x$. Here's an example.

\proclaim Proposition \advthm. There exists an integer $m>0$ and an integer $n<0$ such that $m^2 + n^2 = 25$.

\noindent We've written the statement in words, and we will continue to do so for all the propositions
in these notes because we are humans and not cyborgs,
but notice that the underlying predicate logical formula here is
$$\exists m\,\exists n : m>0 \wedge n< 0 \wedge m^2 + n^2 = 25,$$
with $U = \ZZ$. For the remainder of this section, we'll continue to write out the formulaic equivalents
of propositions, to practise converting between the two worlds.

\proof After a moment's contemplation, we notice that $9 + 16 = 25$. So we need a positive integer that
squares to to $9$ and a negative number that squares to $16$. Hence we may pick $m = 3$ and $n=-4$,
and $m^2 + n^2 = 25$.\slug

On the other hand, to prove a statement of the form $\forall x : P(x)$, we are not allowed to choose the
value of $x$. Instead, we imagine it is given to us, and we still have to prove $P(x)$, no matter what
the $x$ might be. Here's what we mean.

\proclaim Proposition \advthm. For all $x\in \QQ$, there exists $y\in \ZZ$ such that
$xy \in \ZZ$.

\noindent The formula this time is
$$\forall x\, \exists y : y\in \ZZ \wedge xy\in \ZZ,$$
over $U = \QQ$.

\proof Let $x \in \QQ$. Then $x$ can be expressed as the ratio of two integers; write $x = m/n$ with
$m,n\in \ZZ$. Then, setting $y=n\in \ZZ$, we have $xy = (m/n)\cdot n = m\in \ZZ$.\slug

Notice how we introduced the variable $x$ in the above proof.
Because we're trying to prove a ``for all'' statement, we use
the word ``let,'' to indicate that $x$ is given to us by some higher power. In fact, we should sort
of view this higher power as possibly being malicious. A very useful way to think about writing a proof is
to imagine a game in which you are trying to prove a statement in predicate logic, and a supernatural adversary
is attempting to thwart you. Let's say this predicate has four variables, so the statement is
$$\exists x \, \forall y \, \forall z\, \exists w : P(x,y,z,w).$$
The variables in the statement are introduced from left to right. Each time you see a $\exists$
symbol, it's your turn. In the example above, you get to set the variable $x$ to any element of the
given universe (keeping in mind that your eventual goal is to prove $P(x,y,z,w)$). Each time there
is a $\forall$ symbol, it's the adversary's turn, and you should be prepared for whatever he throws at you.
So in our example, the adversary may set $y$ and $z$ to anything in the universe, and he knows you picked
$x$. Lastly, you get to pick $w$. If $P(x,y,z,w)$ is true, you win, and if not, the adversary wins.
Writing a proof is equivalent to describing a winning strategy for the player against the adversary.

\medskip\noindent\boldlabel\datestamp{17}{ix} Proving conditional statements.
Many mathematical statements introduce some hypotheses, then assert some conclusion. Thus they are some
kind of statement of the form $p\To q$. To prove this kind of statement,
we assume that $p$ holds, then prove that $q$ is true. This is because the only way a statement for $p\To q$
to be false is if $p$ is true and $q$ is false, so we're showing this cannot happen. Take a look at
the following example.

\edef\propoddimpliessquareodd{\the\thmcount}
\proclaim Proposition \advthm. If $n$ is an odd integer, then $n^2$ is also odd.

\noindent
The underlying formula is $\forall n\bigl( (\exists k : n= 2k+1) \To (\exists l : n^2 = 2l+1)\bigr)$,
in the universe $U=\ZZ$.

\proof Let $n$ be an arbitrary odd integer, so that there exists $k$ such that $n = 2k+1$. Then
$$n^2 = (2k+1)^2 = 4k^2 + 4k + 1 = 2(2k^2 + 2k)+1,$$
so setting $l = 2k^2 + 2k$, which is an integer, we have $n^2 = 2l+1$. Hence $n^2$ is odd.\slug

To disprove a statement, we simply prove that its negation is true. For example, suppose we want
to disprove the statement $\exists x\, \forall y : x+y = 0$ (this was a statement we encountered
in the previous section). Its negation is
$$\forall x\, \exists y : x+y \ne 0,$$
and to prove it we let $x$ be given, set $y = -x + 1$, and observe that $x+y = x + (-x+1) = 1$, which is
not equal to $0$.

Now we define two words that sound similar but are very different, logically speaking.
The {\it converse}
of a conditional statement $p\To q$ is the statement $q\To p$. The
statement $p\To q$ and its converse both hold if and only if
the biconditional statement $p\Leftrightarrow q$ holds.
On the other hand, the {\it contrapositive} to $p\To q$ is the statement
$\neg q \To \neg p$. What's the deal with this silly-looking conditional? Well, it turns out to actually
be equivalent to $p\To q$. Check it out:
$$\eqalign{
p\To q &\equiv \neg p \vee q \cr
&\equiv q \vee \neg p \cr
&\equiv \neg (\neg q) \vee \neg p \cr
&\equiv \neg q \To \neg p \cr
}$$
So to prove a statement of the form $p\To q$, it is sufficient to prove $\neg q\To \neg p$. Sometimes,
this can make the proof a lot easier. Here's an example. (It is the converse of
Proposition~{\propoddimpliessquareodd}.)

\edef\propsquareoddimpliesodd{\the\thmcount}
\proclaim Proposition \advthm. If $n$ is an integer such that $n^2$ is odd, then $n$ is odd.

\noindent{\it Proof attempt.}\enspace Assume $n^2$ is odd, so that $n^2 = 2k+1$ for some integer $k$.
Then $n = \sqrt{2k+1}$.
\medskip

Where do we go from here? It's not even clear that $\sqrt{2k+1}$ is an integer, let alone an odd one.
The contrapositive comes to the rescue.

\medskip\noindent{\it Proof of Proposition~{\propsquareoddimpliesodd}.}\enspace
We proceed by contraposition.
Suppose that $n$ is not odd; that is, $n$ is even. So there
exists an integer $k$ such that $n=2k$. Then $n^2 = (2k)^2 = 4k^2$. Setting $l = 2k^2$, we have
$n^2 = 2l$, so $n^2$ is even. Hence $n^2$ is not odd.\slug

\medskip\boldlabel Proofs by contradiction. We now discuss a powerful method of proof, which dates
back to at least the
ancient Greeks and referred to for much of Western history by its Latin name,
{\it reductio ad absurdum}. It proceeds to prove
a statement $p$ by assuming its negation $\neg p$
holds, then deriving a contradiction, i.e., a statement whose truth value is $0$.
By doing this, one will have proved that $\neg p\To 0$, which in turn shows that $p$ is true, since
$$\neg p \To 0 \equiv \neg(\neg p) \vee 0 \equiv p\vee 0 \equiv p.$$
Here is an example of a proof by contradiction.

\proclaim Proposition \advthm. There is no least positive rational number.

\noindent In $U = \QQ$, one way to formulate this is
$$\neg \bigl(\exists x: x > 0 \wedge (\forall y : y > 0 \To x \le y)\bigr).$$

\proof Suppose, towards a contradiction, that there exists some rational $x>0$ such that for all $y\in \QQ$
with $y > 0$, $x\le y$. Then, we can apply this property with $y = x/2$ to see that $x \le x/2$.
Since $x$ is positive, we can divide this inequality by $x$ on both sides to obtain $1\le 1/2$. But this
is absurd, since $1/2 < 1$. This contradiction completes the proof.\slug

Here is another classical example of a proof by contradiction. It was known to the Pythagoreans.

\edef\thmsqrttwo{\the\thmcount}
\proclaim Theorem \advthm. The number $\sqrt 2$ is irrational.

\proof Suppose, towards a contradiction, that $\sqrt 2$ is rational. Then there are integers $p$ and $q$
such that $\sqrt 2 = p/q$, and furthermore, we can assume that $p$ and $q$ do not have any common factors,
since if they did, we could divide them out and the ratio would remain the same.
Squaring both sides of the equation, we have $2 = p^2/q^2$, or in other words, $2q^2 = p^2$. This implies
that $p^2$ is even, so by (the contrapositive of) Proposition~{\propsquareoddimpliesodd}, $p$ is even as well.
That means we can write $p=2r$ for some integer $r$, and substitute this new information
into the above equation to obtain $2q^2 = (2r)^2 = 4r^2$. Dividing out by $2$ yields $q^2 = 4r^2$,
so $q^2$ is even, and so is $q$.

We have deduced that $p$ and $q$ are both even. On the other hand, we assumed they had no common factors.
This is a contradiction.\slug

Earlier we described the composition of a proof as playing a game with a supernatural adversary.
To prove something by contradiction, then is is akin to starting the game
by letting adversary believe he has already won,
and then working from that assumption to derive an impossibility. Quite the devious strategem.

\bigskip
\begingroup\obeylines\eightssi
\hfill {\eightss Reductio ad absurdum}, which Euclid loved so much,
\hfill is one of a mathematicians finest weapons.
\hfill It is a far finer gambit than any chess gambit:
\hfill a chess player may offer the sacrifice of a pawn
\hfill or even a piece,
\hfill but a mathematician offers the {\eightss game}.
\eightss
\smallskip
\hfill --- G.~H.~HARDY, {\eightssi A Mathematician's Apology} (1940)
\endgroup%\obeylines
\bigskip\goodbreak

\medskip\boldlabel Case analysis.
For all $n\ge 2$ and all propositions $p_1, \ldots, p_n, q$, we have the equivalence
$$(p_1\vee p_2 \vee \cdots \vee p_n) \To q \equiv (p_1\To q) \wedge (p_2\To q) \wedge \cdots \wedge
(p_n\To q).$$
(As an exercise, prove this when $n=2$. Later on, when we learn about mathematical induction,
you'll be able to prove it in general.)

Why is this useful? Well, if we have two or more propositions $p_1, p_2, \ldots, p_n$
such that at least one of them must hold, that is,
$$p_1\vee p_2 \vee \cdots \vee p_n \equiv 1,$$
then if we are able to show that
$$p_1 \To q,\quad p_2\To q,\quad \ldots,\quad\hbox{and}\quad p_n\To q$$
all hold, then we will have shown
$$1\To q \equiv \neg 1 \vee q \equiv 0 \vee q \equiv q.$$
Let's see an example of this in action.

\proclaim Proposition \advthm. For all integers $n\ge 0$, $1 + (-1)^n (2n-1)$ is a multiple
of $4$.

\proof Let an integer $n\ge 0$ be given. We know that $n$ is even or $n$ is odd.

If $n$ is even, then $n = 2k$ for some integer $k$, and
$$1 + (-1)^{2k} \bigl( 2(2k)-1\bigr) = 1 + 1^k (4k-1) = 4k,$$
which is a multiple of $4$.

If $n$ is odd, then $n=2k+1$ for some integer $k$, and
$$1 + (-1)^{2k+1}\bigl(2(2k+1) - 1\bigr) = 1- (4k+2-1) = 1-(4k+1) = -4k,$$
which is also a multiple of $4$.\slug

\noindent
\datestamp{19}{ix} \hskip\parindent Sometimes, the cases into which one might split the proof are not so obvious.

\proclaim Proposition \advthm. There exist irrational numbers $a$ and $b$ such that $a^b$ is rational.

\proof Consider the number $(\sqrt 2)^{\sqrt 2}$. It is either rational or irrational.

If it is rational, then we can set $a = b = \sqrt 2$, which we already know to be irrational, by
Theorem~{\thmsqrttwo}.

If it is irrational, then we can set $a =(\sqrt 2)^{\sqrt 2}$ and $b=\sqrt 2$. We compute
$$a^b = \bigl((\sqrt 2)^{\sqrt 2}\bigr)^{\sqrt 2} = (\sqrt 2)^{\sqrt 2 \cdot \sqrt 2}
= (\sqrt 2)^2 = 2,$$
which is rational.\slug

Note that in this proof, we did not need to know whether $(\sqrt 2)^{\sqrt 2}$ is rational or not;
we just know that it must either be rational or irrational.
(In fact, it is known that $(\sqrt 2)^{\sqrt 2}$ is irrational, but the usual proof of this
requires some notions from complex analysis as well as Galois theory, subjects that are
typically not encountered until at least the second or third year of a mathematics degree.)

\medskip\boldlabel Mathematical induction. Sometimes we want to prove an infinite
number of statements, indexed by the natural numbers. If the complexity of the statements
sort of ``grows'' in $n$ (a vague notion to be made precise soon), the following theorem holds.

\parenproclaim Theorem {\advthm} (Principle of mathematical induction). Let $P(n)$ be a family of predicates
indexed by $n\in \NN$. Let $m\in \NN$. If
\medskip
\item{i)} $P(m)$ holds; and
\smallskip
\item{ii)} for all $n\ge m$,
$$\bigl( P(m)\wedge P(m+1)\wedge \cdots \wedge P(n) \bigr) \To P(n+1)$$
\medskip
then $P(n)$ holds for all $n\ge m$.

In the following proof, we use the {\it well-ordering principle}, which is the
fact that every nonempty subset of the natural numbers has
a least element. You can see the proof of the well-ordering principle in a higher-level set theory course.

\proof Assume that (i) and (ii) both hold. Our goal is to prove that $P(n)$ is true for all
$n\ge m$, so we shall suppose, towards a contradiction, that there is some $n\ge m$ such that
$\neg P(n)$ holds. (As an exercise, convince yourself, via symbolic manipulations, that
$$\neg \forall n : n\ge m \To P(n) \equiv \exists n : n\ge m \wedge \neg P(n).)$$
In other words, the set of $n\ge m$ such that $\neg P(n)$ holds is nonempty, so it has a least element,
call it $k$. So $k\ge m$, and we cannot have $k=m$ due to (i), so $k>m$. Furthermore, by the minimality
of $k$, the statements $P(m)$, $P(m+1)$, all the way up to $P(k-1)$ are true. By (ii) though,
this implies that $P(k)$ holds: a contradiction.\slug

When proving something by induction, we need to prove two things: one of the form (i) and another
of the form (ii). The former is called the {\it base case} and the latter the {\it induction step}
or {\it inductive step}. The assumption to the left of the $\To$ symbol in (ii)
is called the {\it induction hypothesis} or {\it inductive hypothesis}.
Technically, the theorem above is called the principle of {\it strong} induction;
the principle of {\it weak} induction has
\medskip
\item{ii)$'$} for all $n\ge m$, $P(n)\To P(n+1)$,
\medskip
instead of the stronger (ii). It turns out that both types of induction are actually equivalent,
so we'll use both interchangeably. More often than not, the hypothesis (ii)$'$ is perfectly sufficient,
and in these cases we'll simply use weak induction so as not to clutter our proofs with lots of unused
hypotheses.

Here is an example. Suppose we want to find (and prove) a formula for the sum of the first $n$ odd numbers.
The first thing to do is to try some small cases. When $n = 1$, the sum is just $1$. When $n=2$,
the sum is $1+3 = 4$, when $n=3$, we have $1+3+5 = 9$, and for $n=4$, we compute $1+3+5+7 = 16$.
So the pattern goes $1,4,9,16,\ldots$, which leads us to conjecture that the sum of the first $n$
odd numbers might equal $n^2$. In fact, this is true, and we shall prove it by induction.

\proclaim Proposition \advthm. For all integers $n\ge 1$,
$$\sum_{i=1}^n (2i-1) = n^2.$$

\proof By induction on $n$. First we prove the base case, $n=1$. We have
$$\sum_{i=1}^1 (2i-1) = 2-1 = 1 = 1^2.$$
Now for the inductive step. Let $n\ge 1$ and assume that
$$ \sum_{i=1}^n (2i-1) = n^2.$$
Then
$$\eqalign{
\sum_{i=1}^{n+1} (2i-1) &= \sum_{i=1}^n (2i-1) + \bigl(2(n+1) - 1\bigr) \cr
&= n^2 + 2n+2 - 1 \cr
&= n^2 + 2n + 1\cr
&= (n+1)^2,\cr
}$$
where it is in the second line that we used the inductive hypothesis.\slug

The first thing to notice about doing a proof by induction is that the proof method itself doesn't
tell you what it is you should prove. You have to guess at the correct statement first. Also,
proofs by induction are often ``unenlightening,'' in that they often don't reveal the fundamental reasons
why something might be true. (The previous proposition can be illustrated by a rather simple
picture, which is not a proof, but is somewhat more elucidating that the induction proof.)

A longer example now. Suppose we have a pile of $n$ stones, with $n\ge 1$. We have a job, which
can be described by a pseudo-algorithm.

\algbegin Algorithm S (Divide stones). The input to
this algorithm is an integer $n\ge 1$, representing a number of stones.
We have a list {\tt PILES} of integers representing a collection
of piles of stones, as well as an integer variable {\tt BALANCE}.
Initialise ${\tt PILES} \Gets 1$ (one pile with $n$ stones), and set ${\tt BALANCE}\gets 0$.
This algorithm splits the stones into $n$ piles of $1$ stone each, accumulating profits
into {\tt BALANCE} along the way.
\algstep S1. [Done?] If every element of {\tt PILES} is $1$, terminate the algorithm and output
{\tt BALANCE}.
\algstep S2. [Choose a pile.] Select some element $m>2$
from {\tt PILES} and remove it from the list. (The variable $m$ is the number of stones in this pile.)
\algstep S3. [Split.] Let $k$ and $l$ be two numbers with $k+l = m$. We append ${\tt PILES}\Gets k$ and
${\tt PILES} \Gets l$, and increment ${\tt BALANCE} \gets {\tt BALANCE} + k\cdot l$. (We have split
pile $n$ into two piles of size $k$ and $l$, and the payout for doing so is $k\cdot l$ dollars.)
\algstep S4. [Loop.] Go to step S1.\slug

This is not, strictly speaking, an algorithm, since we didn't specify how the algorithm
should choose the integers $k$ and $l$ that add up
to $m$ in step S3. However, running through a few instances with, say, $n=6$, on paper, using whatever
choices of split you like in every iteration of step S3, you'll find that the algorithm always terminates
with ${\tt BALANCE} = 15$. Trying a few different starting values of $n$ might lead you to conjecture
the following proposition, which we will prove by (strong) induction.

\proclaim Proposition \advthm. For a given input $n$, Algorithm S always outputs ${\tt BALANCE} = n(n-1)/2$,
regardless of the choice of split at any given iteration of step S3.

\proof By induction on $n$. For the base case $n=1$, note that we immediately output ${\tt BALANCE} = 0$
in the very first step of the algorithm, and $0 = 1(1-1)/2$.

Now for the inductive step, let $n\ge 1$ and suppose that for $1\le k \le n$, the payout for running
algorithm S on input $l$ is $l(l-1)/2$. Suppose we have a pile of $n+1$ stones. We shall divide
it into piles of size $k$ and size $n+1-k$. The total payout will be the pay for this division,
namely $k(n+1-k)$, plus the pay for further subdividing the two piles. Thus by the induction
hypothesis applied twice, the total payout will be
$$\eqalign{
k(n+1-k) + &{k(k-1)\over 2} + {(n+1-k)(n-k)\over 2} \cr
&= {2nk + 2k - 2k^2 + k^2 - k + n^2 + n -2nk - k + k^2\over 2} \cr
&= {n^2 + n\over 2} \cr
&= {(n+1)\bigl((n+1)-1\bigl)\over 2},\cr
}$$
which is the expected formula for $n+1$.\slug

\advsect Functions

A {\it function} $f$ from a set $A$ to a set $B$ is a subset $f\subseteq A\times B$ such that
for every $a\in A$, there is exactly one $b\in B$ such that $(a,b)\in f$. (If there is no $b\in B$,
or more than one, we say that $f$ is not well-defined.
If $(a,b)\in f$,
we write $f(a) = b$. The set $A$ is called the {\it domain} and the set $B$ is the {\it codomain}.
The notation $f:A\to B$ is a way of concisely writing ``$f$ is a function with domain $A$ and
codomain $B$.''

Here are some examples and non-examples.
\medskip
\item{i)} The function $f:\RR\to\RR$ given by $f(x) = x^2$ is
$$f = \bigl\{ (a,b)\in \RR^2 : b = a^2 \bigl\},$$
when written in set-builder notation.
\smallskip
\item{ii)} On the other hand, the set
$$g = \bigl\{ (a,b)\in \RR^2 : a = b^2 \bigl\}$$
is not a function, since $(1,1)$ and $(1,-1)$ are both in $g$. Furthermore,
there is no element of $g$ with $-1$ as its first coordinate.
\smallskip
\item{iii)} If $X = \{1,2,5\}$ and $Y = \{0,1,2,3,4,5\}$, then
$$\bigl\{ (1,0), (2, 4), (5,5)\bigr\}$$
is a function from $X$ to $Y$.
\smallskip
\item{iv)} The set
$$h = \bigl\{ (x,y)\in \RR^2 : y = 1/x \bigl\}$$
is not a function, since there is no $y$ such that $(0,y)\in h$. However
if we amend the domain and consider
$$h = \bigl\{ (x,y)\in \bigl(\RR\setminus \{0\}\bigr)\times \RR : y = 1/x \bigl\},$$
then in fact, $h:\RR\setminus \{0\} \to \RR$ is a function.
\medskip

\medskip\boldlabel\datestamp{24}{ix} Injective and surjective functions.
The {\it range} or {\it image} of a function $f:A\to B$ is
$$f(A) = \bigl\{b\in B : \hbox{there exists}\ a\in A\ \hbox{such that}\ b = f(a)\bigr\}.$$
These are all the values $f$ actually outputs. For instance, if we let $f : \ZZ\to \NN$
be given by $f(n) = n^2$, then
$$\eqalign{
f(\ZZ) &= \{\ldots, f(-2), f(-1), f(0) , f(1), f(2),\ldots\} \cr
&= \{\ldots, 9, 4, 1, 0 1, 4, 9,\ldots\} \cr
&= \{0, 1, 4, 9, 16, 25, \ldots\}. \cr
}$$
A function $f:A\to B$ is called {\it surjective} or {\it onto} if $f(A) = B$, that is,
for every $b\in B$ there exists some $a\in A$ such that $f(a) = b$. An example of a
surjective function is $f:\QQ\to \QQ$ sending $x\mapsto x/2$. This is because for any $q\in \QQ$,
we can set $r = 2q$, and
$$f(r) = {r\over 2} = {2q\over 2} = q.$$
A function $f:A\to B$ is called {\it injective} or {\it one-to-one} if for all $a_1,a_2\in A$
with $a_1 \ne a_2$, we also have $f(a_1)\ne f(a_2)$. Equivalently, $f$ is injective if
$f(a_1) = f(a_2)$ implies that $a_1 = a_2$ for all $a_1, a_2 \in A$. For instance, the function
$f:\ZZ\to \NN$ that sends $n\mapsto n^2$ is not injective since $f(-1) = f(1)$, but $-1 \ne 1$.
On the other hand, if we modify the domain, considering $f:\NN\to \NN$ sending $n\mapsto n^2$,
then now $f$ is injective, since if $f(m) = f(n)$, then $m^2 = n^2$, and there is only one positive
integer that squares to any given integer, so $m=n$.

Hence we see that any function can be transformed into an injective one, in principle, by shrinking its
domain (though this new function might no longer have the properties you liked in the original one),
and any function $f:A\to B$ can be made surjective by changing its codomain to its range, i.e.,
letting $B = f(A)$.

\medskip\boldlabel The pigeonhole principle. We now take a brief pause to introduce one of
the most fundamental laws in discrete mathematics, called the pigeonhole principle.
We begin with the following intuitive theorem.

\proclaim Theorem \advthm. Let $a_1, a_2, \ldots, a_n$ be a finite sequence (repeats allowed) of
real numbers. Let
$$a = {1\over n}\sum_{i=1}^n a_i$$
be the average value of the sequence and let $m$ be the maximum value the sequence attains. Then $m \ge a$.

\proof We have
$$a = {1\over n}\sum_{i=1}^n a_i \le {1\over n} \sum_{i=1}^n m = {mn\over n} = m.\noskipslug$$

This theorem can be summed up in one sentence: {\sl The maximum is at least the average.}
Don't underestimate this theorem even though its proof was a one-liner! It is often used to prove
highly nontrivial results.
(As an exercise, prove the similar statement: {\sl The minimum is at most the average.})
From here we are now equipped to prove (a general version of) the pigeonhole principle.

\edef\thmgeneralpigeonhole{\the\thmcount}
\proclaim Theorem \advthm. Let $A$ and $B$ be finite sets with $|A| = m$ and $|B|=n$.
For every function $f:A\to B$, then there is some $b\in B$ such that there are at least
$\lceil m/n\rceil$ elements $a\in A$ with $f(a) = b$.

\proof Enumerate $B = \{b_1, b_2, \ldots, b_n\}$. For $1\le i \le n$, let $r_i$ be the number of $a\in A$
such that $f(a) = b_i$. This is a sequence that adds up to $m$, since every $a$ in $A$ maps to
exactly one element of $B$. So the sequence has average $m/n$, and by the previous theorem,
there must be some $j$ such that $r_j \ge m/n$. But the $r_i$ are all actually integers (since
cardinalities of finite sets are integers), meaning that $r_j \ge \lceil m/n \rceil$. Letting
$b = b_j$ completes the proof.\slug

The reason this is called the pigeonhole principle is because of the following special case.

\parenproclaim Corollary {\advthm} (Pigeonhole principle). Let $n\ge 2$. If $n$ pigeons nest in $n-1$ holes,
there is at least one hole that contains at least two pigeons.

\proof Let $A$ be the set of pigeons and $B$ the set of pigeonholes. Let $f$ be any function sending
the set of pigeons to the set of holes. By the previous theorem, there is some hole with at least
$\lceil n/(n-1)\rceil = 2$ pigeons in it. (This is because $1 < n/(n-1) < 2$ for all integers $n\ge 2$.)\slug

\medskip\boldlabel Bijections.
A function $f$ is called {\it bijective} (or a {\it bijection}, or a {\it one-to-one correspondence})
if it is injective and surjective. Bijections are important because of the following proposition.

\proclaim Proposition \advthm. Let $A$ and $B$ be finite sets. Then
\medskip
\item{i)} there exists a bijection $f:A\to B$ if and only if $|A| = |B|$; and
\smallskip
\item{ii)} if $|A| = |B|$ and $f:A\to B$ then $f$ is injective if and only if $f$ is surjective.
\medskip

\proof Suppose $|A| = |B| = n$. Then choose an ordering $a_1, \ldots, a_n$ of $A$ and an ordering
$b_1, \ldots, b_n$ of $B$. Let $f(a_i) = b_i$ for all $1\le i\le n$. By construction, this is a bijection,
proving one direction of (i).

On the other hand, suppose $|A|\ne |B|$ (so we prove this direction by contraposition).
If $A<B$, then $f$ cannot be surjective, since the image of
$f$ has size at most $|A| < |B|$ (at least one element of $B$ must be missed). If $A>B$, then
$|A|/|B| > 1$, so by Theorem~{\thmgeneralpigeonhole}, there is some element $b\in B$
such that the number of $a\in A$ mapping to $b$ is at least $\lceil |A|/|B| \rceil \ge 2$.
This means that $f$ is not injective. We have proved part (i).

To prove part (ii), let $|A| = |B|$ and let $f:A\to B$. First we assume that $f$ is injective.
We enumerate $A = \{a_1, a_2, \ldots, a_n\}$. Then
$$f(A) = \bigl\{ f(a_1), f(a_2), \ldots, f(a_n)\bigr\} \subseteq B.$$
All of the $f(a_i)$ are distinct, since if $f(a_i) = f(a_j)$, then $a_i = a_j$. So
$\bigl| f(A)\bigr| = |A| = n$, and $f(A)$ is a size $n$ subset of $B$, which also has size $n$.
Hence $f(A) = B$; that is, $f$ is surjective.

Lastly, suppose $f$ is not injective (again we are using contraposition). So there
are $a_i$ and $a_j$ such that $a_i\ne a_j$ but $f(a_i) = f(a_j)$. So
$$\bigl|f(A)\bigr| = \bigl|\bigl\{ f(a_1), f(a_2), \ldots, f(a_n)\bigr\}\bigr| < n = |B|,$$
so $f(A) \ne B$ and $f$ is not surjective.\slug

Item (i) of the previous proposition should be entirely intuitive, especially if we use the alternative
nomenclature ``one-to-one correspondence'' instead of ``bijection.'' (In fact, we already implicitly
used (i) in these notes, in the proof of Proposition~{\propcardpowerset}.) Item (ii) is perhaps not
as immediate, but should become clear if you work it out with a picture.

\medskip\boldlabel Bijections.
A function $f:A\to B$ is called {\it invertible} if there exists $g:B\to A$ such that
\medskip
\item{i)} for all $b\in B$, $f\bigl(g(b)\bigr) = b$; and
\smallskip
\item{ii)} for all $a\in A$, $g\bigl(f(a)\bigr) = a$.
\medskip
If $g$ exists, it can be shown that $g$ must be unique, so we write $g = f^{-1}$ and speak of
{\it the} inverse of $f$.

\proclaim Proposition \advthm. Let $f:A\to B$ be a function. Then $f$ is invertible
if and only if $f$ is bijective.

\proof First we assume that $f$ is invertible. So there exists an inverse $g$ of $f$. For each $b\in B$,
setting $a = g(b)$ we have
$$f(a) = f\bigl(g(b)\bigr) = b.$$
This proves that $f$ is surjective. To show that $f$ is injective, suppose that $f(a_1) = f(a_2)$.
By applying $g$ on both sides, we have $g\bigl(f(a_1)\bigr) = g\bigl(f(a_2)\bigr)$, whence
$a_1 = a_2$, by definition of $g$.

Now assume that $f$ is bijective. We need to define $g:B\to A$. Well, gven any $b\in B$, there is
some $a$ such that $f(a) = b$, from surjectivity of $f$, and this $a$ is unique, since $f$ is
injective. So set $g(b) = a$ (and repeat this process for every $b\in B$). We have
$f\bigl(g(b)\bigr) = f(a) = b$, and for every $a\in A$, by definition of $g$ the
element $g\bigl(f(a)\bigr)$ is the unique element in $A$ that gets brought to $f(a)$ by $f$,
has to be $a$ itself.\slug

Sometimes to prove that two sets have the same cardinality, it is easier to prove that there
exists a bijection (as we already saw in the example of Proposition~{\propcardpowerset}),
and sometimes to prove that a function is a bijection, it is easier to show that it has an inverse,
rather than messing around with the definitions of injective and surjective.
Here's an example.

\proclaim Proposition \advthm. Let $X$ be a finite nonempty set. Let $E$ be the set of all subsets of $X$ with
even cardinality, and let $F$ be the set of all subsets of $X$ with odd cardinality.
Then $|E| = |F|$.

\proof We shall construct a function $f:E\to F$. Fix one specific $x\in X$;
we can do this because $X\ne \emptyset$. Now, for all $A\in E$, let
$$f(A) = \cases{A\setminus\{x\}, & if $x\in A$;\cr A\cup \{x\}, & if $x\notin A$.\cr}$$
Note that since $|A|$ is even for all $A\in E$, the cardinality of $f(A)$ is odd (in the first
case it is $|A|-1$ and in the second case it is $|A|+1$. This shows that $f$ is indeed a function
with codomain $F$. Now to prove $|E| = |F|$ we will show that $f$ is bijective, which we
do by showing that $f$ is an inverse (as an exercise, you might instead try to prove bijectivity from
the definitions of injective and surjective).

We define $g : F\to E$ by
$$g(A) = \cases{A\setminus\{x\}, & if $x\in A$;\cr A\cup \{x\}, & if $x\notin A$.\cr}$$
As before, $\bigl|g(A)\bigr|$ is even, since $A$ is assumed to be a member of $F$ now. Now
for any $A\in E$,
$$\eqalign{
g\bigl(f(A)\bigr) &= \cases{g\bigl(A\setminus\{x\}\bigr), & if $x\in A$;\cr
g\bigl(A\cup \{x\}\bigr), & if $x\notin A$\cr} \cr
&= \cases{\bigl(A\setminus\{x\}\bigr)\cup \{x\}, & if $x\in A$;\cr
\bigl(A\cup \{x\}\bigr)\setminus \{x\} , & if $x\notin A$\cr} \cr
&= \cases{A, & if $x\in A$;\cr
A, & if $x\notin A$\cr} \cr
&= A.\cr
}$$
The proof that $f\bigl(g(A)\bigr) = A$ is similar. Thus $g$ is the inverse of $f$.\slug

\advsect Cardinality

\datestamp{26}{ix}
Earlier, we defined the cardinality of a set to be the number of elements it contains. What, then,
is the cardinality of $\NN$? How about $\RR$? You might say $\infty$, but this is not a number
(at least, it's not an element of $\NN$, the way all cardinalities of finite sets are).
So perhaps we should amend our question to the following: {\sl When do infinite sets have the same size?}
Our experience with functions leads us to the answer: {\sl When there exists a bijection between
them}. We shall say that $A$ and $B$ are {\it equipotent} (or {\it equinumerous}, or {\it have
the same cardinality}) if there exists a bijection between $A$ and $B$. In this case
we write $|A| = |B|$.

As an example, the sets $\NN$ and $\NN\setminus\{0\}$ are equipotent, since $f$
given by $n\mapsto n+1$ is a bijection $\NN\to\NN\setminus \{0\}$. (Check that
$f^{-1}(m) = m-1$ is its inverse.)

It is even possible to remove an infinite number of elements from $\NN$ and end up with
something still equipotent with $\NN$. To see this, let $E$ be the set of nonnegative even integers,
and consider the function $f :\NN\to E$ sending $n\mapsto 2n$. This is injective because if
$2m = 2n$, then dividing out by $2$ on both sides yields $m=n$. It is surjective because if
$n\in E$, then $n=2k$ for some $k\in\NN$, by definition, and $f(k) = 2k = n$.

So we can find subsets of $\NN$ equipotent with it. It turns out we can also find supersets of $\NN$ with
the same property.
\proclaim Theorem \advthm. We have $|\NN| = |\ZZ|$.

\proof We define $f : \NN\to \ZZ$ by
$$f(n) = \cases{{n\over 2}, & if $n$ is even;\cr {-{n+1\over 2}}, & if $n$ is odd.}$$
We shall show that $f$ is a bijection.

Note first that if $n$ is even, then $f(n)\ge 0$,
and if $n$ is odd, then $f(n) < 0$. So if $f(m) = f(n)$ for some $m,n\in \NN$, then $f(m)$ and $f(n)$ must
either both be negative, or both be nonnegative. Either way, $m$ and $n$
are either both even or they are both odd. If $m$ and $n$ are both even, then from $f(m) = f(n)$ we derive
$${m\over 2} = {n\over 2},$$
whence multiplying by $2$ on both sides we see that $m=n$. If $m$ and $n$ are both odd, then
$$-{m+1\over 2} = -{n+1\over 2},$$
so, multiplying by $-2$ and subtracting $1$ from both sides we have $m=n$ in this case as well.

Now we show that $f$ is surjective. Let $k\in \ZZ$. If $k\ge 0$, then consider $n=2k$. We have
$$f(n) = f(2k) = {2k\over 2} = k.$$
If $k<0$, then consider $n=-2k-1$. (Check that this is an element of $\NN$.) Then
$$f(n) = f(-2k-1) = -{-2k-1+1\over 2} = -{-2k\over 2} = k.$$
This shows that $f$ is surjective, so $f$ is in fact bijective and we conclude that $|\NN|=|\ZZ|$.\slug

We say that a set $A$ is {\it countably infinite} if there exists a bijection $f:\NN\to A$, that is,
if $|\NN| = |A|$.
A set is said to be {\it countable} if it is either finite or countably infinite.
Otherwise it is called {\it uncountable}.
The previous theorem shows that $\ZZ$ is countably infinite.

Sometimes it is difficult to come up with a bijection directly. Instead, we would like to find
an injection from $A$ to $B$ (which, in some sense, shows that $|A|\le |B|$), and then an injection
from $B$ to $A$. This is made possible by the following useful theorem, named for Ernst Schr\"oder
and Felix Bernstein, who independently proved it in 1898. The proof is a bit difficult, so it's
technically outside the scope of the course. For fun, you might try to do it as an exercise.

\parenproclaim Theorem {\advthm}. (Schr\"oder--Bernstein theorem). If there exists an injective
function $f:A\to B$ and another injective function $g:B\to A$, then there is a bijection
$h:A\to B$.

\noindent\llap{*}{\it Proof.}\enspace We present the proof as a (difficult) exercise. Here is the roadmap.
Call $b\in B$ {\it unattached} if there is no $a\in A$ such that $f(a) = b$.
Let $h:B\to B$ be given by $h(b)  = f\bigl(g(b)\bigr)$. Given $b,b'\in B$, we shall say that
$b$ is a {\it peer} of $b'$ if either $b = b'$ or there exists some $n\in \NN$ such
that
$$ b = \underbrace{h(h(\cdots(h}_{n\ \hbox{\sevenrm times.}}(b'))\cdots))$$
Say that $b\in B$ is a {\it PAE} if it is the peer of an unattached element. (So unattached
elements are automatically PAEs, by setting $n=0$.)
\medskip
\item{a)} Show that if $a\in A$ is such that
$f(a)$ is a PAE, then
there is a unique element $b^*\in B$ such that $g(b^*) = a$, and that
this element is a PAE.
\medskip
\noindent By part (a), if $f(a)$ is a PAE, it makes sense to speak of $g^{-1}(a)$. It is
the element $b^*\in B$ such that $g(b^*) = a$. From here we define
$$ r(a) = \cases{ g^{-1}(a), & if $f(a)$ is a PAE;\cr
f(a), & otherwise.}$$
\medskip
\item{b)} Show that if $b\in B$ is a PAE
then so is $f\bigl(g(b)\bigr)$.
\item{c)} Show that $r$ is surjective. [{\it Hint}: Do a proof by cases. Every $b\in B$ is either
a PAE or it is not a PAE.]
\smallskip
\item{d)} We already proved in part (a) that if $f(a)$ is a PAE, then so is $r(a)$. Prove
the converse of this statement.
\smallskip
\item{e)} Show that $r$ is injective, and therefore bijective. [{\it Hint}:
Assume $r(a_1) = r(a_2)$, and do a proof by cases again. Part (d) will be useful here.]\slug
\medskip

Once again, this proof is outside the scope of the course. Don't lose sleep over it if you can't do it.
Feel free to ask questions at office hours if you get stuck.

Using the Schr\"oder--Bernstein theorem, we now show that the Cartesian product of two countable sets is also
countable.
In the proof, we shall employ the fact that if a set $A$ is countable, then there exists an enumeration
$$A = \{a_0, a_1, a_2, \ldots\}.$$
(If $f$ is the bijection given by the definition, we can set $a_0 = f(0)$, $a_1 = f(1)$, and so on.)

In the following proof, we'll also use the Fundamental Theorem of Arithmetic, which we state now, and prove
later in the course (in the number theory section). It says that
any integer can be factored as a product of primes, a theorem you should have learned in grade school.

\edef\thmfta{\the\thmcount}
\parenproclaim Theorem {\advthm} (Fundamental Theorem of Arithmetic). Every positive
integer $n\ge 2$ can be factored into a product
$${p_1}^{v_1} {p_2}^{v_2}\cdots {p_m}^{v_m},$$
where $m\ge 0$ is an integer, $p_1,p_2,\ldots,p_m$ are distinct primes, and
$v_1,v_2,\ldots,v_m$ are positive integers.
This factorisation is unique up to the order of the primes.\slug

\edef\thmcartesiancountable{\the\thmcount}
\proclaim Theorem \advthm. If $A$ and $B$ are countably infinite sets,
then $A\times B$ is also countably infinite.

\proof
Enumerate $A = \{a_0, a_1, \ldots\}$ and $B = \{b_0, b_1, \ldots\}$. Now given
$(a_i, b_j)\in A\times B$ for some $i,j\in \NN$, we can let
$$f(a_i, b_j) = 2^i3^j.$$
We show that this defines an injective function. Suppose that $f(a_i, b_j) = f(a_{i'}, b_{j'}).$
Then $2^i3^j = 2^{i'} 3^{j'}$, and by the uniqueness of prime factorisations in the Fundamental
Theorem of Arithmetic, we see that $i = i'$ and $j=j'$.

Now we produce an injection $g : \NN\to A\times B$ by simply setting
$$g(n) = (a_n, b_0).$$
It is clear this is injective, since if $g(m) = g(n)$, then $(a_m, b_0) = (a_n, b_0)$, and
that implies that $m=n$.\slug

\edef\corzedtwocountable{\the\thmcount}
\proclaim Corollary \advthm. We have $|\ZZ\times \ZZ| = |\NN|$.

\proof
We proved earlier that $\ZZ$ is countably infinite, so we may apply the previous theorem with $A = B = \ZZ$.\slug

This corollary concerning $\ZZ\times \ZZ$ allows us to prove that $\QQ$ is countable as well.

\proclaim Theorem \advthm. The set $\QQ$ of rational numbers is countable.

\proof We define $f:\QQ\to \ZZ\times \ZZ$ as follows. For an element $q\in \QQ$, we let $q = a/b$ the fraction $q$
written in lowest terms (where $a\in \ZZ$ and $b\in \NN\setminus\{0\}$). Then we set $f(q) = (a,b)$.
This is an injective function because if $f(q) = f(q')$, then writing $q = a/b$ and $q' = a'/b'$ in lowest
terms, we have $(a,b) = (a',b')$, so $q = a/b = a'/b' = q'$.

To define an injection $g: \ZZ\times \ZZ\to \QQ$, we recycle the injection we had from the proof
of Theorem~{\thmcartesiancountable}. We already know that $\ZZ$ is countable, so fix an enumeration
$\ZZ = \{a_0, a_1, a_2, \ldots\}$. Then let $g(a_i, a_j) = 2^i 3^j$. The range of $g$ is a subset of $\NN$,
so {\it a fortiori} it is a subset of $\QQ$, and we already showed before that it is an injection.

We have shown that $|\QQ| = |\ZZ\times \ZZ|$, which in turn shows that $|\QQ| = |\NN|$, after
applying Corollary~{\corzedtwocountable}.\slug

So far, we have just given lots of examples of countably infinite sets. Finally, we give an example
of a set that is not countably infinite.

\proclaim Theorem \advthm. The set $A$ of all infinite binary strings is uncountable.

\proof Certainly $A$ is not finite, so we need to show that $|\NN|\ne A$.

Let $f : \NN\to A$. We shall show that $f$ is not surjective. For all $m,n\in \NN$, let $a_{m,n}$
be the $n$th bit of $f(m)$. Consider the infinite binary string
$$s = (1-a_{0,0},1-a_{1,1}, 1-a_{2,2}, 1-a_{3,3}, \ldots).$$
This string $s$ cannot equal $f(m)$ for any $m\in \NN$, since given an arbitrary $m$,
the $m$th bit of $f(m)$ is $a_{m,m}$, whereas the $m$th bit of $s$ is $1-a_{m,m}$. Hence there is some
$s\in A$ such that $f(m)\ne s$ for all $m\in \NN$. So $f$ is not surjective.

Since there can be no surjection $f:\NN\to A$, {\it a fortiori} we cannot have a bijection $\NN\to A$.
We conclude that $|A| \ne |\NN|$.\slug

The proof above was published by Georg Cantor in 1891, and hence is known as {\it Cantor's diagonal argument}.
The technique has since been used to prove many other things.

The set of all infinite binary strings is in bijection with the set of all real numbers in the interval
$[0,1)$. If we're being extra pedantic, we need to forbid an infinite trailing string of all $1$s,
since, e.g., $0.0111\ldots = 0.1000\ldots$ after carrying the $1$s (akin to the fact
that, e.g., $0.0999\ldots = 0.1$). After dealing with this detail, one will have shown that
$\bigl| [0,1]\bigr| \ne |\NN|$.

\advsect Relations

\datestamp{01}{x} A {\it relation} on a set $X$ is a subset $R\subseteq X\times X$. If $(a,b)\in R$
we write $aRb$ and say ``$a$ is related to $b$.'' Here are some examples.
\medskip
\item{i)} The set $L = \bigl\{(a,b)\in \RR\times\RR : a<b\bigr\}$ is a relation. For instance, we can
write $(2,\pi)\in L$, or $2L\pi$, or $2<\pi$ (which is the more common notation for this relation).
\smallskip
\item{ii)} The set $E = \bigl\{ (a,b) \in \RR\times \RR : a = b\bigr\}$ is a relation (it is the ``equals''
relation).
\smallskip
\item{iii)} On $\ZZ$, the set
$$R = \bigl\{(-1, 4), (8, -3), (0, 0), (0,1)\bigr\}$$
is a relation.
\smallskip
\item{iv)} For any set $A$, a function $f:A\to A$ is by definition a subset of $A\times A$, and hence
is an example of a relation.
\smallskip
\item{v)} Let $H$ be the set of all humans, define $M\subseteq H\times H$ by setting
$(h_1, h_2)$ if and only if $h_1$ is married to $h_2$. For instance,
$$(\hbox{Michelle Obama}, \hbox{Barack Obama})\in M.$$
On the other hand, it is unfortunately the case that
$$(\hbox{Marcel Goh}, \hbox{Taylor Swift})\notin M.$$
\medskip

\medskip\boldlabel Properties of relations. Let $R\subseteq A\times A$ be a relation. $R$ is called
\medskip
\item{i)} {\it reflexive} if for all $a\in A$, $aRa$;
\smallskip
\item{ii)} {\it symmetric} if for all $a,b\in A$ with $aRb$, we also have $bRa$; and
\smallskip
\item{iii)} {\it transitive} if for all $a,b,c\in A$ with $aRb$ and $bRc$, we also have $aRc$.
\medskip
Going back to the examples we had above, the relation $L$ is transitive but neither reflexive nor symmetric,
the relation $E$ satisfies all three properties, the relation $R$ satisfies none of them, and the relation
$M$ is symmetric but neither reflexive nor transitive. (Convince yourself of all of these facts.)

As a more involved example, let $X$ be any set with $|X|\ge 2$. On $2^X$, define a relation $R$ by
$$(A,B)\in R \Leftrightarrow A\cap B\ne \emptyset$$
for all $A,B\subseteq X$. Which of the three properties above does $R$ satisfy?

Is it reflexive? Well, is it true that for all $A\subseteq X$, $A\cap A \ne \emptyset$? The answer
is no, since we have $\emptyset \cap \emptyset = \emptyset$, so we have $(\emptyset, \emptyset)\notin R$.

Is $R$ symmetric? Well, if $A\cap B \ne \emptyset$, then since $\cap$ is commutative, we have
$B\cap A \ne \emptyset$ as well, so $(A,B)\in R$ implies that $(B,A)\in R$. In other words, yes, $R$ is symmetric.

Is $R$ transitive? If $A\cap B\ne \emptyset$ and $B\cap C\ne \emptyset$, does that necessarily mean
that $A\cap C\ne\emptyset$? The answer is no. Here's the proof. Since $|X|\ge 2$ we can find $x,y\in X$
with $x\ne y$. Let $A = \{x\}$, $B = \{x,y\}$, and $C = \{y\}$. Then $A\cap B = \{x\}\ne \emptyset$,
$B\cap C = \{y\}\ne \emptyset$, but alas $A\cap C = \emptyset$.

\medskip\boldlabel Equivalence relations and classes. If $R$ is reflexive, symmetric, and transitive, then we say
that $R$ is an {\it equivalence relation}. Here are two examples (as an exercise, prove that both of
these are equivalence relations).
\medskip
\item{i)} Let $F$ be the set of all formulas in propositional logic with the variables $p$, $q$, and $r$,
and operations $\neg$, $\wedge$, and $\vee$. The relation $\equiv$ defined by $f_1 \equiv f_2$
if and only if $f_1$ and $f_2$ have the same truth table is an equivalence relation.
\smallskip
\item{ii)} On the set $\RR^2$, define the relation $R \subseteq \RR^2\times \RR^2$ by letting
$(x,y) R (w,z)$ if and only if $\sqrt{x^2 + y^2} = \sqrt{w^2 + z^2}$. This is also an equivalence relation
\medskip
If $R$ is an equivalence relation, often we shall write $x\sim y$ to mean $xRy$. Sometimes we might
even just say that $\sim$ {\it is} the equivalence relation.

Let $R\subseteq A\times A$ be an equivalence relation. Define the {\it equivalence class} of $a\in A$
to be the set
$$[a] = \{b\in A : a\sim b\}.$$
This is the set of all $b\in A$ that are related to $a$.

Take for instance the example $F$ above of all formulas under the relation $\equiv$. The equivalence class
of the formula $p\vee\neg p$ is the set of all formulas whose truth table contains only $1$s, that is,
the set of all tautologies.
And for the example $R$ above, the
equivalence class of the point $(1,3)$ is the set
$$\bigl[ (1,3)\bigr] = \bigl\{ (x,y)\in \RR^2 : \sqrt{10}= \sqrt{x^2 + y^2}\bigr\};$$
that is, the circle of radius $\sqrt{10}$ centred about the origin.

We have the following proposition concerning equivalence relations.

\proclaim Proposition \advthm. Let $R$ be an equivalence relation on $A$. Then
\medskip
\item{i)} for all $x\in A$, $x\in [x]$;
\smallskip
\item{ii)} for all $x,y\in A$, $x\sim y$ if and only if $[x] = [y]$; and
\smallskip
\item{iii)} for all $x,y\in A$, $x\not\sim y$ if and only if $[x] \cap [y] =  \emptyset$.
\medskip

\proof Let $x\in A$. Since $R$ is reflexive, $x\sim x$, so we have $x\in [x]$. This proves part (i).

For part (ii), first we prove the ``only if'' direction. Let $x,y\in A$ be such that $x\sim y$. To prove that
$[x]\subseteq [y]$, we let $z\in [x]$; so $x\sim z$. By symmetry, we have $z\sim x$, and this
combined with $x\sim y$ allow us to deduce that $z\sim y$, by transitivity. Then by symmetry again,
we have $y\sim z$, so $z\in [y]$. {\it Mutatis mutandis}, i.e., by swapping the roles of $x$ and $y$,
we also have $[y]\subseteq [x]$. Hence $[x] = [y]$.

Now for the ``if'' direction of (ii). Let $x,y\in A$ be such that
$[x] = [y]$. By (i), we have $x\in [x]$, but
then this means that $x\in [y]$. By definition of $[y]$, this means that $y\sim x$, so $x\sim y$
by symmetry.

On to part (iii). We prove both implications by contraposition (that is, we negate both sides of the
statement). Let $x,y\in A$ be such that $[x]\cap [y] \ne \emptyset$. This means there is some $z\in A$ such
that $z\in [x]$ and $z\in [y]$; so $x\sim z$ and $y\sim z$. By symmetry, $z\sim y$,
so by transitivity, we have $x\sim y$.

On the other hand, suppose that $x,y\in A$ satisfy $x\sim y$.
By (ii), we have $[x] = [y]$, so $[x] \cap [y] = [x] \ne \emptyset$,
where we know that $[x]\ne \emptyset$ because it contains at least $x$ (again, using (i)).\slug

Let $A$ be a set. A {\it partition} of $A$ is a set $P$ of subsets
of $A$ (i.e., $P\subseteq 2^A$), such that
\medskip
\item{i)} for all $x\in A$ there exists $S\in P$ such that $x\in S$;
\smallskip
\item{ii)} for all $S_1, S_2\in P$ with $S_1\ne S_2$, the intersection $S_1\cap S_2$ is empty; and
\smallskip
\item{iii)} $\emptyset\notin P$.
\medskip
Here are some examples.
\medskip
\item{i)} Let $E$ be the set of all even integers and $F$ the set of all odd integers. Then $\{E,F\}$
is a partition of $\ZZ$.
\smallskip
\item{ii)} The set $\bigl\{ (-\infty,0), \{0\}, (0,\infty)\bigr\}$ is a partition of $\RR$.
\medskip

\noindent\datestamp{03}{x} \hskip\parindent
If $\sim$ is an equivalence relation on a set $A$, then we can define the {\it quotient}
of $A$ by $\sim$ as the set of all equivalence classes of $A$ under $\sim$. We denote this set by
$$A/{\sim} = \bigl\{ [x] : x\in A\bigr\}.$$
We use the previous proposition to prove that quotients of sets by equivalence relations
are partitions.

\proclaim Proposition \advthm. Let $A$ be a set and $\sim$ an equivalence relation on $A$.
Then $A/{\sim}$ is a partition of $A$.

\proof By part (i) the previous proposition, every $x\in A$ belongs to the equivalence class $[x]$.
Then, by part (ii) of the previous proposition, we know that for any equivalence classes
$[x]$ and $[y]$ such that $[x]\ne [y]$, we must have $x\not\sim y$, and by part (iii) of the previous
proposition, we deduce that $[x]\cap [y]=\emptyset$. This shows that $A/{\sim}$ satisfies the second
part of the definition of partition. Lastly, we note that $\emptyset\notin A/{\sim}$, since
every element of $A/{\sim}$ is equal to $[x]$ for some $x\in A$, and must thus contain at least
the element $x$.\slug

Let us now revisit the examples of equivalence relations from last class, and see what partitions
they give rise to. In the example $F$ of propositional formulas with variables $p$, $q$, and $r$,
under the equivalence relation $\equiv$, the set of equivalence classes is the set of all possible
truth tables on three variables. Each such truth table has $8$ rows, so there are $2^8$ equivalence
classes. In other words, $|F/{\equiv}| = 2^8$.

How about the relation $R$ defined on $\RR^2$ where
$$(x,y)\sim (w,z) \qquad\hbox{if and only if}\qquad \sqrt{x^2+y^2} = \sqrt{w^2 + z^2}?$$
Well, each $\bigl[(x,y)\bigr]$ is the circle of radius $\sqrt{x^2 + y^2}$ centred
around the origin, so $\RR^2/{\sim}$ is the set of all circles in the plane centred at $(0,0)$.


\vfill\eject
\begingroup\headline{\hfil}\footline{\hfil}
\vglue160pt
\centerline{\titlefont II. NUMBER THEORY}
\vskip220pt
\bigskip
\begingroup\obeylines\eightssi
\hfill Die Mathematik ist die K\"onigin der Wissenschaften
\hfill und die Zahlentheorie ist die K\"onigin der Mathematik.
\eightss
\smallskip
\hfill --- {\eightssi Attributed to} C. F. GAUSS {\eightssi (1777--1855)}
\endgroup%\obeylines
\bigskip\goodbreak
\vfill\eject

\advsect Division

Let $a,b\in \ZZ$. We say that $a$ {\it divides} $b$ (or $b$ is a {\it multiple} of $a$, or $b$ is {\it divisible}
by $a$, or $a$ is a {\it factor} of $b$) if there exists $n\in \ZZ$ such that $b=na$. In this
case we write $a\divides b$. For example, $2\divides 10$, since $10 = 5\cdot 2$, but
$3$ does not divide $10$, since there does not exist $n\in \ZZ$ such that $10 = 3n$.
This defines a relation on $\ZZ$.

Note that for all $n\in \ZZ$, $n\divides 0$, since $0 = 0\cdot n$. We also have
$1\divides n$ for all $n\in \ZZ$< since $n = n\cdot 1$. It is true that $0\divides 0$, since we
have, say, $0 = 1\cdot 0$, but for all nonzero $n\in \NN$, $0$ does not divide $n$, since all multiples
of $0$ equal $0$ (and thus cannot equal $n$). Further properties of the ``divides'' relation
are given by the next proposition.

\proclaim Proposition \advthm. For all $a,b,c,d\in \ZZ$,
\medskip
\item{i)} if $a\divides b$, then $a\divides bc$;
\smallskip
\item{ii)} if $a\divides b$ and $a\divides c$, then $a\divides (b+c)$;
\smallskip
\item{iii)} if $a\divides b$ and $b\divides c$, then $a\divides c$;
\smallskip
\item{iv)} if $a\divides b$ and $b\ne 0$, then $|a|\le |b|$; and
\smallskip
\item{v)} if $a\divides b$ and $b\divides a$, then $|a| = |b|$.
\medskip

\proof We leave parts (i) and (ii) as exercises to the reader.

For part (iii), if $a\divides b$ and $b\divides c$,
then there are integers $k$ and $l$ such that $b = ka$ and $c = lb$. Then $c = kla$, so $a\divides c$
(since $kl$ is also an integer).

For part (iv), suppose that $a\divides b$ and $b\ne 0$. Then $b = ka$ for some $k\in \ZZ$,
and $k\ne 0$ since $b\ne 0$. This means that $|b| = |k|\cdot |a|$, but $|k|\ge 1$,
so $|b|\ge |a|$.

For part (v), assume that $a\divides b$ and $b\divides a$. If $a\ne 0$ and $b\ne 0$, then we may apply
part (iv) twice to get $|a|\le |b|$ and $|b|\le |a|$, which together imply $|a| = |b|$. If $b=0$,
then $0\divides a$ so $a = 0$ as well, and $|a| = |b|$. Likewise, if $a = 0$, then $0\divides b$,
so $b=0$ and in this case as well, $|a| = |b|$.\slug

In grade school, you learned how to divide an integer by another one, obtaining a quotient and a remainder.
We state this as a theorem. Its proof (which we shall consider outside the scope of the course,
but which we include as optional reading for those interested)
relies on the well-ordering principle (which we've used
already in these notes)
and the {\it Archimedean property} of $\RR$, which states that for every $x\in \RR$ there
exists $n\in \NN$ such that $n > x$. (One can learn the proof of the Archimedean property from an introductory
course in analysis, e.g., {\mc MATH} 242/254.)

\parenproclaim Theorem {\advthm} (Division algorithm). Let $a,b\in \ZZ$ with $b> 0$. Then there
exist unique integers $q$ and $r$ such that $a = bq+r$ and $0\le r\le |b|$.

\noindent\llap{*}{\it Proof.}\enspace First we show that such integers $q$ and $r$
exist. If $b\divides a$ then $a = kb$ for some $k\in \ZZ$,
and we can set $q = k$ and $r = 0$.

If $a$ is not divisible by $b$, then consider the numbers
$$ \ldots, a - 3b, a-2b, a-b, a, a+b, a+2b, a+3b, \ldots.$$
Let $S$ be the set of these integers that are positive. Symbolically, we have
$$ S = \{a-kb : k\in \ZZ\ \hbox{and}\ a-kb\ge 0\}.$$
By the Archimedean property, there is some $n\in \NN$ such that $n> -a$, which
implies that $nb \ge n > -a$ (here we use the fact that if $b>0$ and $b$ is an integer, then
$b\ge 1$). From this we derive $a > -nb$, and hence $a+nb = a - (-n)b > 0$. This shows
that $S$ is nonempty.

Since $S$ is a nonempty subset of $\NN$, by the well-ordering principle it has a least
element, call it $r$. By definition of $S$, there must be some integer $q$ such that
$r = a-qb$, so $a = bq + r$. We now claim that $0<r<b$.

We know that $r> 0$, since all elements of $S$ are positive by definition. Suppose,
for a contradiction, that $r\ge b$. Then $a - bq = r \ge b$, and so
$$0 \le r-b = a - qb - b = a-(q+1)b.$$
Since $q+1$ is an integer, by definition of $S$, either $r-b$ is an element of $S$,
or $r-b = 0$. Since $r$ was defined to be the minimal element of $S$, it cannot be the
case that $r-b$ is in $S$. So $r-b = 0$. But this means that $0 = a-(q+1)b$; that is,
$a = (q+1)b$, contradicting our assumption that $b$ does not divide $a$. The contradiction
allows us to conclude that $0<r<b$ (in the case that $b$ does not divide $a$). In general,
we have shown that $0\le r<b$.

\endgroup%\headline{\hfil}\footline{\hfil}
Lastly, we need to prove that $q$ and $r$ are uniquely determined by the integers $a$ and
$b$. Suppose that
$$a = bq_1 + r_1\qquad\hbox{and}\qquad 0\le r_1 < b$$
and
$$a = bq_2 + r_2\qquad\hbox{and}\qquad 0\le r_2 < b,$$
for some integers $q_1$, $q_2$, $r_1$, and $r_2$. We shall show that $q_1 = q_2$
and $r_1 = r_2$. Well, suppose that $r_1\ne r_2$, for a contradiction. Without loss
of generality we can assume that $r_1 < r_2$. Then subtracting the two equations, we obtain
$$ 0 = a-a = (bq_1 + r_1) - (bq_2 + r_2) = b(q_1-q_2) + (r_1 - r_2).$$
This means that
$$r_2 - r_1 = b(q_1 - q_2),$$
so we find that $b\divides (r_2 - r_1)$. By part (iv) of the previous proposition,
we obtain $|b| \le |r_2-r_1|$, and we can simply write $b \le r_2 -r_1$, since both of
these quantities are positive. But this is a contradiction, since
$$ 0\le r_1 < r_2 < b,$$
yields $r_2 - r_1 < b$. The contradiction shows that that $r_2 = r_1$. Substituting this
into the relation $r_2 - r_1 = b(q_1 - q_2)$, we get $0 = b(q_1 - q_2)$ and conclude that
$q_1 - q_2 = 0$, since $b>0$.\slug

Let $a$ and $b$ be integers, not both zero. Their {\it greatest common divisor}, $\gcd(a,b)$
is defined to be the greatest positive integer $d$ such that $d\divides a$ and $d\divides b$.
Note that $\gcd(0,0)$ is not defined, since all positive integers $d$ satisfy $d\divides 0$.
On the other hand $\gcd(x,0)$ is simply $|x|$, and $\gcd(x,1) = 1$. Lastly, we don't
need to worry about negative signs when computing greatest common divisors; i.e.,
$\gcd(\pm x, \pm y) = \gcd\bigl(|x|, |y|\bigr)$.

\medskip\boldlabel Euclid's algorithm.
Now we ask ourselves, How do we compute greatest common divisors in general? The answer
lies in one of the oldest algorithms known to humankind. It appears in Euclid's {\sl Elements},
written around 300~{\sc b.c.}

\algbegin Algorithm E (Euclid's algorithm). Given two nonnegative integers $a$ and $b$, not both zero,
this algorithm outputs $\gcd(a,b)$.
\algstep E1. If $b=0$, then output $a$ and terminate the algorithm.
\algstep E2. Since $b\ne 0$, by the division algorithm we my write $a = qb+r$, where $0\le r < b$.
Set $a\gets b$, $b\gets r$, and return to step E1.\slug

The algorithm will eventually terminate, since the stopping criterion is that $b$ be equal to $0$,
and in step E2 we replace $b$ with a number that is strictly closer to $0$. But will it terminate
with the correct answer? Well, we know step E1 is correct,
because of our earlier observation that $\gcd(a,0) = a$ (whenever $a$
is positive). On the other hand, it is not at all evident that step E2 will eventually output
$\gcd(a,b)$, since we actually overwrite the values of $a$ and $b$ in the step! The following lemma
clarifies the situation.

\proclaim Lemma \advthm. Let $a,b,q,r\in \ZZ$ be integers such that $a = qb+r$. Then
$\gcd(a,b) = \gcd(b,r)$.

\proof We shall show that for any $d\in \ZZ$,
$$ d\divides a\ \hbox{and}\ d\divides b\qquad\hbox{if and only if}\qquad
d\divides b\ \hbox{and}\ d\divides r.$$
For the forward implication, suppose that $a = kd$ and $b=ld$ for some $k,l\in \ZZ$. Substituting
this into the identity $a=qb+r$ yields $kd = ldq+r$, whence $r = c(k-lq)$, so we conclude that $c\divides r$.
(This is because $k-lq\in \ZZ$.)

For the reverse implication, suppose that $b = ld$ and $r = md$ for some $k,l\in \ZZ$. Substituting
this into $a=qb+r$, we have $a = ldq + md$, so $a = d(lq+m)$, which means that $c\divides a$.

We have proved that $a$ and $b$ have the same common factors as $b$ and $r$, so they must have the
same greatest common divisor.\slug

Now that we are secure in the fact that Euclid's algorithm will indeed terminate with the correct output,
let us now see it in action. Suppose we want to find $\gcd(30,112)$. We perform successive divisions
replacing the pair $(a,b)$ with a new pair $(b,r)$ each time:
\edef\eqeuclid{\the\eqcount}
$$\eqalign{
30 &= 0\cdot 112 + 30 \cr
112 &= 3\cdot 30 + 22 \cr
30 &= 1\cdot 22 + 8 \cr
22 &= 2\cdot 8 + 6 \cr
8 &= 1\cdot 6 + 2 \cr
6 &= 3\cdot 2 + 0\cr
}\adveq$$
We stop once the remainder $r$ equals $0$, and the answer is $\gcd(b,r) = \gcd(b,0) = b$. (So in the
example above, the final answer is $2$.)

\medskip\boldlabel B\'ezout's identity.
The following theorem allows us to express the greatest common divisor as a linear combination
of the two integers in question.

\edef\thmbezout{\the\thmcount}
\parenproclaim Theorem {\advthm} (B\'ezout's identity). Let $a$ and $b$ be nonzero integers
with greatest common divisor $\gcd(a,b)$. Then there exist integers $s$ and $t$ such that
$$\gcd(a,b) = sa+tb.$$
Moreover, $\gcd(a,b)$ is the least positive integer that can be expressed as an
integer linear combination of $a$ and $b$.

\noindent\datestamp{08}{x} {\it Proof.}\enspace
Let
$$S = \{ s'a+t'b : s',t'\in \ZZ,\ ax+by > 0\}.$$
This set is nonempty, since if $a$ is negative then $(-1)a+0b\in S$ and if $a$ is positive,
then $1a+0b\in S$. Since $S$ is a nonempty set of positive integers, it has
a least element, by the well-ordering principle. Call this integer $d = sa+tb$ (for some
specific choices of $s,t\in \ZZ$);
the claim is that
$d = \gcd(a,b)$.

By the division algorithm, we may write
$$a = dq+r$$
where $q$ and $r$ are integers with $0\le r<d$. But we can write
$$\eqalign{
r &= a-qd \cr
&= a - q(sa+tb) \cr
&= (1-qs)a + (qt)b,\cr
}$$ so $r\in S\cup \{0\}$. But $r < d$, so if $r\in S$, then $d$ would not be the smallest
element of $S$. So we must have $r = 0$. This implies that $d$ is a divisor of $a$.
Repeating this argument with $b$ instead of $a$, we find that $d$ divides $b$ as well.

We have shown that $d$ is a common divisor of $a$ and $b$. It remains to show that it is the greatest
one. That is, we must show that if $c\divides a$ and $c\divides b$, then $c\le d$. Well, if
$a = kc$ and $b=lc$, then the identity
$$d = sa+tb = skc + tlc = (sk+tl)c,$$
shows that $d$ is a multiple of $c$ as well. Since $d>0$, this means that $c\le d$.\slug

This theorem is named for \'Etienne B\'ezout, who proved an analogous result (with polynomials
instead of integers) in 1779, but the result above for integers has been known since at least
the 1600s.

To actually find the integers $s$ and $t$ such that $\gcd(a,b) = sa + tb$, we
first perform the Euclidean algorithm, keeping track of all our intermediate steps. Then we combine
all the information from each step to work out what $s$ and $t$ are.
For example, in the earlier example showing that $2 = \gcd(112,30)$, we start with
$$ 2 = 8- 1\cdot 6,$$
which is (a rearrangement of) the fifth line of~\refeq{\eqeuclid}.
Then the fourth line of~\refeq{\eqeuclid} says that $6=22-2\cdot 8$, so
$$ 2 = 8 - 1\cdot (22-2\cdot 8) = 8 - 22 + 2\cdot 8 = 3\cdot 8 - 22.$$
The third line of~\refeq{\eqeuclid} tells us that $8 = 30-1\cdot 22$, which gives
$$ 2 = 3\cdot (30-1\cdot 22) - 22 = 3\cdot 30 - 3\cdot 22 - 22 = 3\cdot 30 - 4\cdot 22.$$
We now have the number $30$ appearing in the expression, we just need to get rid of the $22$
and replace it with $112$. To do this we use the second line of~\refeq{\eqeuclid}, which
says that $22 = 112-3\cdot 30$. We end up with
$$ 2= 3\cdot 30 - 4(112-3\cdot 30) = 3\cdot 30 - 4\cdot 112 + 12\cdot 30 = (-4)\cdot 112 + 15\cdot 30.$$
So in the case that $a = 112$ and $b=30$, we have $d = 2$, $s = -4$, and $t = 15$.

We now summarise this section on B\'ezout's identity with a little scenario.
Imagine a frog that lives on a doubly-infinite line of lilypads,
indexed by the integers. It starts at the point $0$
and can hop in steps of $a$ or $b$ (in either direction). Theorem~{\thmbezout} tells us
that the lilypad $d = \gcd(a,b)$ is reachable by the frog. The next proposition characterises
the set of {\it all} lilypads that the frog can get to.

\proclaim Proposition \advthm. Let $a$ and $b$ be nonzero integers. The set
$$X = \{ s'a + t'b : s',t'\in \ZZ\}$$
is exactly the set of multiples of $d = \gcd(a,b)$.

\proof By B\'ezout's identity, there exist integers $s$ and $t$ such that $d = sa+tb$.
First let $n\in \ZZ$ be a multiple of $d$. Then there is $k\in \ZZ$ such that $n=kd$,
and we have
$$ n = kd = d(sa+tb) = (ds)a + (dt)b,$$
which means that $n\in X$ (since $ds$ and $dt$ are both integers).

Conversely, suppose that $n\in X$, so $n = s'a + t'b$ for some $s',t'\in \ZZ$. Then since
$d$ divides $a$ and $d$ divides $b$, we can write $a = ld$ and $b=md$ for some integers $l,m\in \ZZ$.
So we have
$$ n= s'a + t'b = s'ld + t'md = (s'l+t'm)d,$$
which shows that $d\divides n$, since $s'l+t'm$ is an integer.\slug

As a corollary, if $\gcd(a,b) = 1$, then it is possible for the robot to reach every integer!
The situation in which $\gcd(a,b) = 1$ is very special, so much so that we have a name for it.
We say that integers $a$ and $b$ are {\it relatively prime} or {\it coprime} if $\gcd(a,b) = 1$.
By B\'ezout's identity, $a$ and $b$ are relatively prime if and only if
there are integers $s$ and $t$ such that $1 = sa + tb$. This gives a very quick and easy way to
check if certain numbers are relatively prime. For example, we have the following proposition.

\proclaim Proposition \advthm. For all integers $n>1$, $n$ and $n+1$ are relatively prime.

\proof We have $1= 1(n+1) + (-1)n$.\slug

\advsect Primes

An integer $p$ is {\it prime} if $p\ge 2$ and for all $d\in \NN$ with $d\divides p$,
we either have $d=1$ or $d = p$. An integer $n$ is {\it composite}
if $n\ge 2$ and $n$ is not prime. (By negating the definition of prime, we
see that $n\ge 2$ is composite if and only if there exist $a,b\in \{2,\ldots,n-1\}$ such that
$n = ab$.) Note that the integers $0$ and $1$ are neither prime nor composite.

The following theorem gives another characterisation of prime numbers.

\edef\thmpdivides{\the\thmcount}
\proclaim Theorem \advthm. An integer $p$ with $p\ge 2$ is prime if and only if
for all $a,b\in \NN$, $p\divides ab$ implies that $p\divides a$ or $p\divides b$.

\proof First we prove the forward implication.
Suppose that $p$ is prime and let $a,b\in \NN$ be such that $p\divides ab$. So
there exists an integer $k$ such that $ab = kp$.
Consider $\gcd(a,p)$. Since the
only divisors if $p$ are $1$ and $p$, this must be $1$ or $p$. If it is $p$,
then $p\divides a$ and we are done. So we restrict our attention to the case that
$\gcd(a,p) =1$, and our goal is to prove $p\divides b$. By B\'ezout's identity,
there are integers $s$ and $t$ such that $1= sa+tp$, so multiplying both sides by $b$,
we have
$$ b =bsa+btp = skp + btp = (sk+bt)p,$$
so $p\divides b$.

We prove the reverse implication by contraposition.
Now suppose that $p$ is composite. So there exist integers $2\le a,b\le p-1$
such that $p=ab$. We want to show that $p$ does not divide $a$ and $p$ does not divide $b$.
We shall do this by showing that $u = a/p$ and $v = b/p$ are both not integers. Well
since $p = ab = upb$, by dividing through by $p$ we arrive at
$1 = ub$, and since $b\ge 2$, $u = 1/b$ is between $0$ and $1$. This shows that $p$ does
not divide $a$. On the other hand, since $p = ab = apv$, we have $1 = av$,
and since $a\ge 2$, this means that $v = a/1$ is between $0$ and $1$. Hence
$p$ does not divide $b$.\slug

To illustrate that $p$ really does have to be prime for this theorem to hold, consider
$p=6$, $a = 2$ and $b=15$. We have $6 \divides 30 = 2\cdot 15$, but $6$ divides neither $2$ nor $15$.
By induction, the theorem extends to arbitrary finite products.

\proclaim Corollary \advthm. Let $p$ be prime and $n$ be a positive integer. If $a_1,a_2,\ldots,a_n$
are integers such that $p \divides a_1a_2\cdots a_n$, then $p\divides a_i$ for some
$1\le i\le n$.

\proof By induction on $n$. When $n=1$, there is nothing to prove,
for if $p\divides a_1$, then $p\divides a_i$ for $i = 1$.

Now suppose the statement holds for $n$. Assume that $p\divides a_1a_2\ldots a_n a_{n+1}$.
By setting $a = a_1a_2\cdots a_n$ and $b=a_{n+1}$, we have $p\divides ab$, so by the previous theorem,
either $p\divides a$ or $p\divides b$. If $p\divides b$, then $p\divides a_i$ for $i=n+1$,
and if $p\divides a$, then $p\divides a_1\cdots a_n$, so by the induction hypothesis,
$p\divides a_i$ for some $1\le i\le n$.\slug

Back in Section~6, we used the prime factorisation of integers in a proof, but didn't prove
that statement itself. It's finally time to do so. Recall that we statement we used
is that any integer $n$ can be factored into a product
$$n = {p_1}^{v_1} {p_2}^{v_2} \cdots {p_k}^{v_k}$$
where $p_1 < p_2<\cdots<p_k$ are primes and $v_1,v_2,\ldots,v_n$ are positive integers. By renumbering
the primes, allowing them to possibly equal one another, we have the following equivalent statement.

\parenproclaim Theorem {\thmfta}$'$ (Fundamental Theorem of Arithmetic, again). Every integer
$n\ge 2$ can be expressed as a product
$$n = p_1p_2\cdots p_k$$
where $p_1\le p_2\le \cdots \le p_k$ are prime numbers. Furthermore, this factorisation
is unique.

\proof The proof that such a decomposition of $n$ exists is
by (strong) induction. The base case is $n=2$. This is already a prime factorisation, since $2$ is prime.

For the inductive step, let $n\ge 2$ and assume such a decomposition exists
for all $2\le i\le n$. Now consider $n+1$. If $n+1$ is prime, then by setting $p_1 = n+1$,
we have a prime factorisation of $n+1$ without getting out of bed. If $n+1$ is not prime,
then $n+1 = ab$ for some integers $2\le a,b\le n$. By the induction hypothesis, $a$ and $b$
can be factored into primes; that is, $a = p_1,\ldots,p_l$
and $b = q_1,\ldots,q_m$ for some primes $p_1,\ldots,p_l,q_1,\ldots,q_m$. So
$$n+1 = ab = p_1\cdots p_l q_1\cdots q_m$$
is a factorisation of $n+1$ into primes.
It remains to arrange the $p$s and $q$s in nondecreasing order.

Now we prove that the prime factorisation of an integer is unique. In other words, $n\ge 2$
decomposes into
$$n = p_1p_2\cdots p_s$$
and
$$n = q_1q_2\cdots q_t$$
for primes $p_1,\ldots,p_s,q_1,\ldots,q_t$,
then $s=t$ and $p_i = q_i$ for all $1\le i\le s$. We also prove this statement by induction,
but this time it is on the integer $s$. If $s=1$, then
$$p_1 = n = q_1q_2\cdots q_t.$$
Note that $t$ has to equal $1$ here, since otherwise $q_1$ and $q_2$ would both
divide $p$ and satisfy $2\le q_1,q_2\le p_1$, contradicting the fact that $p_1$ is prime.
So $p_1 = q_1$ and we are done.

Next, suppose that the uniqueness
statement holds for $s$ (i.e., for any integer that decomposes
into a product of $s$ primes, the decomposition is unique) and we want to show that it holds
for $s+1$. We assume that
$$n = p_1p_2\cdots p_{s+1}$$
and
$$n = q_1q_2\cdots q_t,$$
where $p_1\le p_2\le \cdots \le p_{s+1}$ and $q_1\le q_2\le\cdots\le q_t$. We
have $p_{s+1}\divides n = q_1q_2\cdots q_t$, so by the preceding corollary, there
exists $1\le i\le t$ such that $p_{s+1}\divides q_i$, and since $q_i$ is prime, this means
that $p_{s+1} = q_i\le q_t$. Similarly, $q_t$ divides $n = q_1\ldots p_{s+1}$, so it
divides $p_j$ for some $1\le j\le s+1$. This means that $q_t = p_j \le p_{s+1}$. Hence
$q_t = p_{s+1}$. But
$$p_1p_2 \cdots p_s p_{s+1} = q_1q_2 \cdots q_t,$$
so we may divide out by $p_{s+1} = q_t$ on both sides to get
$${n\over p_{s+1}} = p_1p_2\cdots p_s = q_1 q_2 \cdots q_{t-1}.$$
By the induction hypothesis, the decomposition of $n/p_{s+1}$ is unique, so $t-1 = s$
and $p_i = q_i$ for all $1\le i\le t-1$. This proves that $s+1 = t$ and
$p_i = q_i$ for all $1\le i\le s+1$.\slug

\noindent\datestamp{10}{x} \hskip\parindent
We can use the Fundamental Theorem of Arithmetic to prove the following generalisation
of Theorem~{\thmsqrttwo}.

\proclaim Theorem \advthm. Let $k$ and $n$ be positive integers. Then either
$\root k\of n$ is an integer or it is irrational.

\proof We prove this by contraposition, showing that if $\root k\of n$ is rational, then it
must be an integer. Suppose that $\root k\of n$ is rational; so there exist integers
$a$ and $b$ with $b\ne 0$ such that $\root k\of n = a/b$. Without loss of generality,
we may choose $a$ and $b$ such that $\gcd(a,b) = 1$.
Writing
$$a = {p_1}^{v_1} {p_2}^{v_2}\cdots {p_r}^{v_r}$$
and
$$b = {q_1}^{w_1} {q_2}^{w_2}\cdots {q_s}^{w_s},$$
for primes $p_1<p_2<\cdots<p_r$, primes $q_1<q_2<\cdots<q_s$, and positive integers
$v_1,v_2,\ldots,v_r,w_1,w_2,\ldots,w_s$, all the primes $p_i$ and $q_j$ must be different,
since if there was a prime in common, call it $p = p_i = q_j$, we would have $p\divides \gcd(a,b)$,
contradicting the fact that $\gcd(a,b) = 1$.

Taking the identity $\root k\of n = a/b$ to the power of $k$ yields
$$n = {a^k\over b^k} = {{p_1}^{kv_1} {p_2}^{kv_2}\cdots {p_r}^{kv_r}
\over {q_1}^{k w_1} {q_2}^{k w_2}\cdots {q_s}^{k w_s}}.$$
Since there do not exist $1\le i\le r$ and $1\le j\le s$ such that $p_i=q_j$, this
fraction is in reduced form. But since $n$ is an integer, that means the denominator equals $1$.
In other words, $n= a^k$, so $\root k\of n = a$ is an integer.\slug

Let's see how to use this theorem. Suppose we want to know if the number $\root 6\of 18$ is
irrational or not. Well, since $1^6 = 1$ and $2^6 = 64$, we have $1^6 < 18 < 2^6$,
meaning that $1<\root 6\of 18 < 2$. (This is because the function $f:\RR\to\RR$ defined
by $f(x) = x^6$ is increasing on the interval $[1,2]$.) Hence $\root 6\of 18$ is not
an integer, and by the theorem it must be irrational.

Next, we show that there are infinitely many primes, a theorem first proved by Euclid.

\proclaim Theorem \advthm. There are infinitely many prime numbers.

\proof Suppose, towards a contradiction, that there are finitely many prime numbers, call
them $p_1, p_2, \ldots, p_m$. Let $n = p_1p_2\cdots p_m$, and consider the integer $n+1$. Either
it is prime or it is not. If $n+1$ is prime, then we already have a contradiction, since $n+1 > p_i$
for all $1\le i\le m$, and we assumed that the $p_1,\ldots,p_m$ were all the primes. If $n+1$
is not prime, then it is divisible by some prime in our list, call it $p_i$. Hence we can write
$n = kp_i$ for some integer $k$, and we have
$$ k p_i = n+1 = p_1 p_2\cdots p_m + 1.$$
Rearranging this a bit, we have
$$ 1 = kp_i - p_1 p_2\cdots p_m = p_i(k - p_1 p_2 \cdots p_{i-1} p_{i+1} \cdots p_m);$$
in other words, $p_i$ divides $1$. This is a contradiction as $1$ is not divisible by any integer
greater than $1$.\slug

Though the set $P$ of prime numbers is infinite, it does sort of get ``sparser'' as one heads off
towards infinity. This is quantified by the following theorem, proved independently
in~1896 by Jacques Hadamard and Charles Jean de la Vall\'ee Poussin.

\parenproclaim Theorem {\advthm} (Prime number theorem). For $x\in \RR$, let
$$\pi(x) = \bigl| \{ p \le x : p\ \hbox{prime}\} \bigr|.$$
Then $\pi(x) \sim x/\ln x$, in the sense that
$$\lim_{x\to\infty} {\pi(x)\over x/\ln x} = 1.$$

The proof is long and arduous, requiring a lot of background in complex analysis.
It is often taught in a first-year graduate class in analytic number theory.
Here is an easy corollary of the prime number theorem.

\proclaim Corollary \advthm. Let $n$ be a positive integer and let $m$ be chosen
uniformly at random from the set $\{1,2,\ldots, n\}$. Then
$$(\ln n)\pr\{m\ \hbox{prime}\}\to 1$$
as $n\to\infty$. In other words, the probability that $m$ is prime is asymptotically
$1/\ln n$.

\proof Since $m$ is chosen uniformly at random from $\{1,\ldots,n\}$,
the probability $\pr\{ m\ \hbox{prime}\}$ equals $\pi(n)/ n$. So
$$\lim_{n\to\infty} (\ln n) \pr\{m\ \hbox{prime}\} = \lim_{n\to\infty} {\pi(n) \ln n\over n} 
= 1,$$
by the prime number theorem.\slug

As an example, if we choose a $30$-digit number at random (so $n=10^{30} - 1$), the
probability that this number is prime is roughly $1/\ln(10^{30}) = 1/(30\ln 10) = 0.0145$
or $1.45\%$.

\advsect Modular arithmetic

Fix $n\ge 1$, and let $a,b\in\ZZ$. We say {$a$ is congruent to $b$ modulo $n$}
if $n\divides a-b$, i.e., if $a-b = kn$ for some $k\in \ZZ$. Write $a \equiv b\pmod n$
or $a \equiv_n b$.

Take for example $n=12$. We have, e.g., $4\equiv 16\pmod 14$, since $4-16 = -12 = (-1)12$,
but, e.g., $7\not\equiv 17$, since $7-17 = -10$, and $12$ does not divide $-10$. This
situation should be a familiar one, since we are used to working with numbers modulo $12$ when
telling the time.

For any fixed $n$, the set of all $(a,b)\in \ZZ^2$ with $a\equiv_n b$ is a relation on $\ZZ$.
In fact, we have the following proposition.

\proclaim Proposition \advthm.
For all fixed $n$, the relation $\equiv_n$ is an equivalence relation on the set $\ZZ$.

\proof We must show that $\equiv_n$ is reflexive, symmetric, and transitive.

Let $a\in \ZZ$. We have $a-a = 0 = 0\cdot n$, so $a\equiv a\pmod n$, proving reflexivity.

Let $a,b\in \ZZ$ and suppose that $a\equiv_n b$, so there exists $k$ such that $a-b=kn$.
Then $b-a = (-k)n$, so $b\equiv_n a$. This proves symmetry.

Lastly, let $a,b,c\in \ZZ$ be such that $a\equiv_n b$ and $b\equiv_n c$. So there
exist integers $k,l\in \ZZ$ such that $a-b = kn$ and $b-c = ln$. Then
$$a - c = (a-b) + (b-c) = kn +ln = (k+l)n,$$
which shows that $a\equiv_n c$.\slug

Since $\equiv_n$ is an equivalence relation, it partitions $\ZZ$ into equivalence classes.
We shall denote by $[a]_n$ the equivalence class of $a$ modulo $n$. This is the set
$$[a]_n = \{b\in \ZZ : a-b = kn\ \hbox{for some}\ k\in \ZZ\}
= \{a + ln : l\in \ZZ\}.$$
For instance, when $n = 3$, the set
$$[0]_3 = \{\ldots, -6, -3, 0, 3, 6, \ldots\}$$
is just the set of all multiples of $3$, and we also have
$$[1]_3 = \{\ldots, -5, -2, 1, 4, 7,\ldots\}$$
and
$$[2]_3 = \{\ldots, -4, -1, 2, 5, 8,\ldots\}.$$
These are all of the equivalence classes, since, for instance,
$$[4]_3 = \{\ldots, -2, 1, 4, 7, 10,\ldots\} = [1]_3.$$

Lastly, we shall touch upon the {\it modulo} operator, which is a feature of many programming
languages. Let $a\in \ZZ$ and $b\ge 1$. Let $a \percent b = r$ where $q$ and $r$ are the integers
given by the division algorithm. (That is, $a = qb+r$ where $0\le r\le b$.)

\proclaim Proposition \advthm. Fix an integer $n\ge 2$.
Let $a,b\in \ZZ$. Then $a\equiv b\pmod n$ if and only
if
$$a\percent n = b\percent n.$$

\proof
Suppose that $a\equiv_n b$, so that there exists $k\in \NN$ such that $a-b = kn$.
Then, let $a = nq_1 + r_1$ and $b = nq_2 + r_2$ from the division algorithm.
(So $r_1 = a\percent n$ and $r_2 = b\percent n$, and we have $0\le r_1,r_2 < n$.)
Write
$$kn = a - b = (q_1-q_2) n + (r_1 - r_2),$$
which we can rearrange to
$$r_1 - r_2 = kn - (q_1 - q_2)n = (k-q_1 + q_2)n.$$
This implies that $r_1 - r_2$ divides $n$, but
since $r_1, r_2\in [0,n)$, the quantity $r_1 - r_2$ is in the range $[-n-1, n)$. Hence
the only way it can divide $n$ is for $r_1 - r_2 = 0$. We conclude that $a\percent n = b\percent n$.

For the other direction, we once again
let $a = nq_1 + r_1$ and $b = nq_2 + r_2$ from the division algorithm.
Now the assumption is that $r_1 = a\percent n = b\percent n = r_2$. So
$$a - b = (q_1-q_2) n + (r_1 - r_2) = (q_1-q_2)n.$$
This shows that $a\equiv b\pmod n$.\slug
\bye

\noindent\datestamp{14}{} \hskip\parindent
This proposition is useful in practice. For example, suppose we want to know whether
$22\equiv 8\pmod 3$. We calculate $22 = 7\cdot 3 + 1$, so $22\percent 3 = 1$; meanwhile
$8 = 2\cdot 3 + 2$, so $8\percent 3 = 2$. By the proposition, this means that
$22\not\equiv 8\pmod 3$.

The proposition also implies that for all $a\in \ZZ$ and $n\ge 2$ one has $[a]_n = [a\percent n]$.
From the division algorithm, we know that $a\percent n$ is an element in the range $[0,n)$;
it is equal to the integer $r$ in that range such that we may write $a = qn+r$ for some
integer $q$. We may choose to denote the whole equivalence class by this element $r$. The set
$$\ZZ/n\ZZ = \ZZ/{\equiv_n} = \bigl\{ [0], [1], \ldots, [n-1] \bigr\}$$
is called the {\it ring of integers modulo $n$} or the {\it cyclic group on $n$ elements}.

\medskip\boldlabel Computations modulo {\mathbold n}.
We can do addition and multiplication modulo $n$ by first doing addition and multiplication
and then taking the remainder with respect to $n$. For example, when $n=4$ we have the
addition table
$$
\vbox{\ninepoint\tabskip7pt\offinterlineskip
\halign{
\hfil $#$ \hfil & \hfil\vrule# \hfil \enskip &
\hfil $#$ \hfil & \hfil $#$ \hfil & \hfil $#$ \hfil & \hfil $#$ \hfil \cr
+ && 0 & 1 & 2 & 3 \cr
& height7pt &&&& \cr
\noalign{\hrule}
& height7pt &&&& \cr
0 && 0 & 1 & 2 & 3 \cr
& height7pt &&&& \cr
1 && 1 & 2 & 3 & 0 \cr
& height7pt &&&& \cr
2 && 2 & 3 & 0 & 1 \cr
& height7pt &&&& \cr
3 && 3 & 0 & 1 & 2 \cr
}}
$$
and the multiplication table
$$
\vbox{\ninepoint\tabskip7pt\offinterlineskip
\halign{
\hfil $#$ \hfil & \hfil\vrule# \hfil \enskip &
\hfil $#$ \hfil & \hfil $#$ \hfil & \hfil $#$ \hfil & \hfil $#$ \hfil \cr
\cdot && 0 & 1 & 2 & 3 \cr
& height7pt &&&& \cr
\noalign{\hrule}
& height7pt &&&& \cr
0 && 0 & 0 & 0 & 0 \cr
& height7pt &&&& \cr
1 && 0 & 1 & 2 & 3 \cr
& height7pt &&&& \cr
2 && 0 & 2 & 0 & 2 \cr
& height7pt &&&& \cr
3 && 0 & 3 & 2 & 1 \cr
}}.
$$
For $x,y\in \ZZ$, if $xy = 0$, then either $x=0$ or $y=0$.
On the other hand, from the assumption $xy \equiv 0\pmod n$ we cannot conclude
in general that $x\equiv 0\pmod n$ or $y\equiv 0\pmod n$. As an example, when $n=4$
and $x=y=2$, neither $x$ nor $y$ is congruent to $0$ modulo $4$, but their product is $0$ modulo $4$.
An element $a\in \ZZ$ with $a\not\equiv 0\pmod n$ is said to be a {\it zero divisor}
if there exists $b\in \ZZ$ with $b\not\equiv 0\pmod n$ such that $ab \equiv 0\pmod n$.

Remember that the numbers in the tables above are elements of $\ZZ/n\ZZ$ (in the case $n=4$),
hence they are not really integers so much as equivalence classes of integers, which are sets!
So it is not immediate that addition and multiplication are well-defined modulo some fixed integer $n$.
For instance, to say that $[3]_4 \cdot [1]_4 = [3]_4$, it is not enough to just observe that
$3\cdot 1 = 3$. We need to show that for any integers $a\in [3]_4$ and $b\in [1]_4$, we have
$a\cdot b\in [3]_4$. This is the essence of the following proposition.

\proclaim Proposition \advthm. Let $a\equiv c\pmod n$ and $b\equiv d\pmod n$. Then
\medskip
\item{i)} $a+b\equiv c+d \pmod n$;
\smallskip
\item{ii)} $ab\equiv cd\pmod n$; and
\smallskip
\item{iii)} $a^m \equiv c^m \pmod n$ for all $m\in \NN$.
\medskip

\proof By assumption, there are integers $k$ and $l$ such that $a-c = kn$ and $b-d = ln$.

To prove (i), we compute
$$(a+b)-(c+d) = a-c + b-d = kn+ln = (k+l)n,$$
so $a+b\equiv c+d\pmod n$.

For part (ii), we compute
$$\eqalign{
ab-cd &= ab-cb+cb - cd \cr
&= b(a-c) + c(b-d) \cr
&= bkn + cln \cr
&= (bk + cl) n ,\cr
}$$
so $ab\equiv cd \pmod n$.

We prove part (iii) by induction. The base case is $m=0$; we have $a^0 = 1 = c^0$,
so $a^0\equiv c^0\pmod n$. Now let $m\ge 0$ and assume that $a^m\equiv c^m\pmod n$.
Combining this with the hypothesis $a\equiv c\pmod n$, we use part (ii) to
conclude that $a\cdot a^m \equiv c\cdot c^m\pmod n$, which simplifies
to $a^{m+1}\equiv c^{m+1}\pmod n$.\slug

We can use this to perform the modulo operation on large numbers without necessarily
knowing the large numbers themselves. For example, let $m=21^{10} - 3\cdot 13 + 14$
and suppose we want to know $m\percent 6$.

First we observe that $14\equiv 2\pmod 6$
and $13\equiv 1\pmod 6$. So
$$ m = 21^{10} - 3\cdot 13 + 14 \equiv_6 21^{10} - 3\cdot 1 + 2 \equiv_6 21^{10} - 1.$$
Now we deal with the exponent by first taking the base of the exponent modulo $6$.
We have $21\equiv 3 \pmod 6$; hence
$$m \equiv_6 3^{10} -1.$$
To reduce the exponent,
we factorise $10=2\cdot 5$, yielding
$$m \equiv_6 (3^2)^5 - 1.$$
But again we can reduce the base here, since $9\equiv_6 3$, which gives us
$$m \equiv_6 3^5-1.$$
Now since the exponent $5$ is prime, we aren't able to reduce it by factorising. However,
we do have $5 = 2+2+1$, and, using once again the fact that $3^2 = 9\equiv_6 3$, we obtain
$$m \equiv_6 3^2\cdot 3^2 \cdot 3 - 1 \equiv_6 3\cdot 3\cdot 3 - 1 \equiv_6 3\cdot 3 - 1
\equiv_6 3- 1 \equiv_6 2.$$

\medskip\boldlabel Inverses modulo {\mathbold n}. Considering $2$ as a rational
number, we say that the inverse of $2$ is $1/2$, since $2\cdot (1/2) = 1$.
We write $2^{-1} = 1/2$. We shall use the same terminology in the ring of integers modulo $n$.
An element $a\in \ZZ$ is said to be {\it invertible modulo $n$} if there exists $b\in \ZZ$
such that $ab \equiv 1\pmod n$. In this case we shall say that $b$ is an {\it inverse} of $a$.
In fact, inverses are unique (in $\ZZ/n\ZZ$).

\proclaim Proposition \advthm. Let $a,n\in \ZZ$ with $n\ge 2$. Then if $ab \equiv 1\pmod n$
and $ac\equiv 1\pmod n$, then $b\equiv c\pmod n$.

\proof We have
$$b \equiv b\cdot 1 \equiv b(ac) \equiv (ab)c \equiv 1\cdot c \equiv c\pmod n.\noskipslug$$

Hence we may speak of {\it the} inverse of an element $a$, which we shall denote by $a^{-1}$.
We have $a^{-1}a \equiv 1 \equiv a a^{-1}\pmod n$.

For instance, when $n=5$, the fact that
$$2\cdot 3 \equiv_5 6\equiv_5 1$$
tells us that $2$ (and $3$, for that matter) are invertible modulo $5$.
When $n=6$, is $2$ still invertible? Here $n$ is small enough that we can just try all the cases.
We have $2\cdot 0 = 0$, $2\cdot 1 = 2$, $2\cdot 2 = 4$, $2\cdot 3 = 6\equiv_6 = 0$
$2\cdot 4 = 8 \equiv_6 2$, and $2\cdot 5 = 10\equiv_6 4$. Since none of these were equivalent
to $1$ modulo $6$, we see that $2$ is not invertible modulo $6$.

It was quite a tedious process to exhaustively show that $2$ is not invertible modulo $6$. The following
theorem gives a much easier-to-use criterion for invertibility.

\proclaim Theorem \advthm. Let $a,n\in \ZZ$ with $n\ge 2$. Then
\medskip
\item{i)} $a$ is invertible modulo $n$ if and only if $\gcd(a,n) = 1$; and
\smallskip
\item{ii)} if $a$ is invertible, then there is a unique integer $b\in [0,n-1]$ such that
$ab\equiv 1\pmod n$. Namely, if
$$1 = sa + tn$$,
then we can set $b = s\percent n$.
\medskip

\proof The proof of (i) is a fairly simple corollary of B\'ezout's theorem.
f $a$ is invertible, then $ab\equiv 1\pmod n$, so there exists $k\in \ZZ$ such that
$1-ab=kn$. Rearranging this, we have $1 = ab+kn$, and by B\'ezout's theorem (specifically,
the final statement of Theorem~{\thmbezout}, this implies that $\gcd(a,n) = 1$.
On the other hand, if $\gcd(a,n) = 1$, then there exists $k\in \ZZ$ such that
$1 = ab + kn$. This means that $1-ab = kn$, so $ab\equiv 1\pmod n$.

For part (ii), suppose that $s,t\in \ZZ$ are such that $1 = sa+tn$. Then
taking this equation modulo $n$, we have
$$ 1 = sa+tn \equiv_n sa + 0t \equiv_n sa.$$
Letting $b = s\percent n$, we have $1 = ba$, since $s$ and $b$ belong to the same
equivalence class modulo $n$, and $b$ is an integer in $[0,n-1]$ by the division algorithm.
This proves existence of such a $b$. To show uniqueness, suppose that $b$ and $c$ are both
integers in $[0,n-1]$ with $ab \equiv 1\pmod n$ and $ac\equiv 1\pmod n$. Then
$ab\equiv ac\pmod n$, and multiplying on both sides by $b$, we have $bab\equiv bac\pmod n$.
But $ba\equiv 1\pmod n$, so cancelling this out in the equation, we have $b\equiv c\pmod n$.
In other words, $n\divides b-c$. But since $b,c\in [0,n-1]$, we have $b-c\in [-n+1, n-1]$,
and the only divisor of $n$ in this range is $0$. Hence $b-c = 0$ and we conclude that $b=c$.\slug

We can use this theorem to systematically find inverses of integers modulo other integers. For instance,
suppose we want to find the inverse of $17$ modulo $20$. First we find $\gcd(20,17)$ by Euclid's
algorithm:
$$\eqalign{
20 &= 1\cdot 17 + 3 \cr
17 &= 5\cdot 3 + 2 \cr
3 &= 1\cdot 2 + 1 \cr
1 &= 1\cdot 1 + 0 \cr
}$$
The fact that $\gcd(20,17) = 1$ tells us that $17$ indeed has an inverse. Now we go backwards to
express $1$ as an integer linear combination of $20$ and $17$:
$$\eqalign{
1 &= 3-1\cdot 2 \cr
&= 3-(17-5\cdot 3) \cr
&= -17 + 6\cdot 3 \cr
&= -17 + 6(20-17) \cr
&= 6\cdot 20 - 7\cdot 17 \cr
}$$
So the inverse of $17$ modulo $20$ is $-7 \equiv_{20} 13$. (We can verify that
$$17\cdot 13\equiv (-3)(-7)\equiv 21 \equiv 1\pmod {20}.$$
As a trick for computations, use the range $-n/2$ to $n/2$ instead of $0$ to $n-1$ to keep the
numbers a bit smaller. It might help you avoid errors.)

Let's find the full list of invertible integers modulo $20$, as well as their inverses. First,
we list the integers in the range $[0,19]$ that are relatively prime with $20$:
$$1,3,7,9,11,13,17,19$$
If some number is in this list, then its inverse must also be in this list (since inverses to
a given element must themselves be invertible).
The elements $1$ and $19$ are their own inverses, since $1\cdot 1 = 1$ and
$19\cdot 19 \equiv_{20} (-1)(-1) = 1$.
We already saw above that $13$ and $17$ are inverses to each other. That leaves $3$, $7$, $9$, and $11$.
First we see that $3\cdot 7 = 21 \equiv_{20} 1$, so $3$ and $7$ are inverses to each other.
Then we note that $9\cdot 9 = 81 \equiv_{20} 1$, so $9$ is its own inverse, leaving us to conclude
that $11$ must also be its own inverse as well---either by the fact that we've ruled
out all other possibilities, or by the computation
$$11\cdot 11 \equiv_{20} (-9)(-9)  = 81 \equiv_{20} 1.$$

\noindent\datestamp{15}{} \hskip\parindent
We now investigate what happens in the particular case where we the modulus $n$ is prime.
In this situation we have the following proposition.

\edef\propinvertiblemodp{\the\thmcount}
\proclaim Proposition \advthm. Let $p$ be prime. Then
\medskip
\item{i)} every $x\in \ZZ$ with $x\not\equiv 0\pmod p$ is invertible modulo $p$; and
\smallskip
\item{ii)} for all $a,b\in \ZZ$ with $ab\equiv 0\pmod p$ one has
$a\equiv 0\pmod p$ or $b\equiv 0\pmod p$.
\medskip

\proof Since $p$ only has factors $1$ and $p$, $\gcd(x,p)$ must either be $1$ or $p$.
But the condition $x\not\equiv 0\pmod p$ implies that $p$ does not divide $x$.
Hence $\gcd(x,p) = 1$, proving part (i).

For part (ii), assume that $ab\equiv 0\pmod p$, so $p\divides ab$. But $p$ is prime, so
by Theorem~{\thmpdivides}, either $p\divides a$ or $p\divides b$. In the first
case we have $a\equiv 0\pmod p$, and in the second case, $b\equiv 0\pmod p$.\slug

\medskip\boldlabel Solving congruences modulo {\mathbold n}.
Suppose we want to solve for all integers $x$ satisfying $x^2 \equiv x\pmod n$, first for $n=6$,
then for $n=7$.

For the case where $n=6$, it suffices to consider $x\in \{0,1,\ldots, n-1\}$,
since if $x^2\equiv x\pmod n$ and $x\equiv a\pmod n$, then $a^2\equiv a\pmod n$.
So we simply try all $x\in \{0,1,\ldots, 5\}$. We have $0^2 = 0$, $1^2 = 1$,
$2^2 = 4 \not\equiv_6 2$, $3^2 = 9\equiv_6 3$, $4^2 = 16\equiv_6 4$, and
$5^2 \equiv_2 (-1)^2 = 1\not\equiv_6 5$. We conclude that $0$, $1$, $3$, and $4$ are solutions
to this congruence modulo $6$, and more broadly, any integer $y$ of the form
$$y = x+6k$$
where $k\in \ZZ$ and $x\in \{0,1,3,4\}$, is a solution $x^2\equiv x\pmod 6$.

Now we tackle the case where $n=7$. In fact, the solution is no different
if $n$ were any other prime.
First we subtract $x$ from both sides, obtaining the congruence $x^2 - x\equiv 0\pmod 7$,
then we factorise the left side to get $x(x-1)\equiv 0\pmod 7$. Now by the previous proposition,
since $7$ is prime if $x(x-1)\equiv 0\pmod 7$, we must have either $x\equiv 0\pmod 7$ or
$x-1\equiv 0\pmod 7$, so the only solutions are $x\equiv_7 0$ and $x\equiv_7 1$ (and anything
in their equivalence classes).

Replacing $7$ with any other prime $p$ in the above paragraph yields a proof of the following
proposition.

\proclaim Proposition \advthm. Let $p$ be a prime. Then $a^2\equiv a\pmod p$ if and only if
$a$ is either congruent to $0$ or $1$ modulo $p$.\slug

Here is a similar proposition.

\edef\propselfinversemodp{\the\thmcount}
\proclaim Proposition \advthm. Let $p$ be a prime and let $a\not\equiv 0\pmod p$ (so
that $a$ is invertible. Then $a \equiv a^{-1}\pmod p$ if and only
if $a$ is either congruent to $1$ or $-1$ modulo $p$.

\proof If $a = 1$ or $a=-1$, then $a^2 = 1$, so $a^2\equiv 1$, and multiplying by $a^{-1}$ on both sides,
we have $a\equiv a^{-1}\pmod p$.

Now suppose that $a\equiv a^{-1}\pmod p$. Multiplying by $a$ on both sides, we get $a^2\equiv 1\pmod p$;
then, subtracting $1$ from both sides and factoring the resulting polynomial, we get
$$(a+1)(a-1)\equiv 0\pmod p.$$
This implies that either $a+1\equiv 0\pmod p$ or $a-1\equiv 0\pmod p$. In the first case, $a \equiv_p -1$,
and in the second case, $a\equiv_p 1$.\slug

We finish off this section with an important theorem, first stated by Pierre de Fermat in~1640.
He did not supply a proof; the first published proof of this theorem was given by
Leonhard Euler in~1736.

\edef\thmflt{\the\thmcount}
\parenproclaim Theorem {\advthm} (Fermat's little theorem). Let $a$ and $p$ be integers with $p$ prime.
If $a\not\equiv 0\pmod p$, then $a^{p-1}\equiv 1\pmod p$.

Before proving this theorem, we first state and prove a lemma. Recall that we define the {\it factorial}
of $n\in \NN$ to be the product $n! = 1\cdot 2\cdots (n-1) n$.

\edef\lemhalfwilson{\the\thmcount}
\proclaim Lemma \advthm. For all prime numbers $p$, the integer $(p-1)!$ is congruent to $-1$ modulo $p$.

\proof If $p=2$, we have $(p-1)! = 1\equiv -1\pmod 2$, and if $p=3$, then $(p-1)! = 2 \equiv -1 \pmod 3$.

For the rest of the proof, assume that $p\ge 5$.
By Proposition~{\propselfinversemodp}, each element in the set $S = \{2,3,\ldots,p-3,p-2\}$
is not its own inverse modulo $p$, so for each element $s\in S$, there is some other element $s'\in S$ with
$s'\ne s$ such that $ss'\equiv 1\pmod p$. This means that
$$ 2\cdot 3 \cdots (p-3)(p-2) \equiv 1\pmod p.$$
From this, we see that
$$(p-1)! = 1\cdot 2\ldots (p-2)(p-1) = \bigl(2\cdot 3 \cdots (p-3)(p-2)\bigr)(p-1)
\equiv_p (p-1)\equiv_p -1.\noskipslug$$

We are now able to prove Fermat's little theorem.

\medskip\noindent{\it Proof of Theorem~{\thmflt}}.\enspace
Since $a\not\equiv 0\pmod p$, it is invertible modulo $p$ by Proposition~{\propinvertiblemodp}.
Denote its inverse by $a^{-1}$.
Let $G = (\ZZ/p\ZZ)\setminus\{0\}$ for short. Define a function $f:G\to G$ by letting $f(x) = a\cdot x$,
where we consider the result modulo $p$ (and hence the result is an element of $\ZZ/p\ZZ$).
This function
is well defined: since $a$ and $x$ are both not zero modulo $p$, their product will be an element of $G$.
If $f(x_1) = f(x_2)$, then
$$a\cdot x_1 \equiv a\cdot x_2\pmod p,$$
so multiplying
both sides by $a^{-1}$, we have $x_1 \equiv x_2\pmod p$.
This proves that $f$ is injective. Now let $y\in G$. We want
to find $x$ with $f(x) \equiv y\pmod p$.
To do so, simply set $x \equiv a^{-1}y\pmod p$. Then
$$f(x) = f(a^{-1}y) = a(a^{-1}y) \equiv y \pmod p.$$
This proves that $f$ is surjective, and hence bijective.

The bijection $f$ shows that the set
$$\bigl\{a, 2a, \ldots,(p-2)a, (p-1)a\bigr\},$$
taken as a subset of $\ZZ/p\ZZ$ (i.e., we take each element modulo $p$, in the range $[0,\ldots p-1]$),
is exactly the same as the set
$$\{1, 2, \ldots, p-2, p-1\},$$
just that the order of elements might be permuted. Hence we have
$$a(2a)\cdots\bigl((p-2)a\bigr)\bigl((p-1)a\bigr) \equiv (p-1)!\pmod p.$$
Combining all the $a$ factors on the left-hand side, we get
$$a^{p-1} (p-1)!\equiv (p-1)!\pmod p.$$
But by the previous lemma, $-1$ is the inverse of $(p-1)!$ modulo $p$,
so multiplying both sides of this congruence by $-1$, we get
$$a^{p-1}\equiv 1\pmod p,$$
which is what we wanted to show.\slug

As a matter of interest, the intermediary lemma we proved is one direction of Wilson's theorem, proved
by John Wilson in 1770.

\parenproclaim Theorem {\advthm} (Wilson's theorem).
For all integers $n\ge 2$, the congruence
$$(n-1)!\equiv -1\pmod n$$
holds if and only if $n$ is prime.

\proof We already proved the ``if'' direction earlier, as Lemma~{\lemhalfwilson}.
We leave the ``only if'' direction of the proof as an exercise for the reader.\slug

\bigskip
\begingroup\obeylines\eightssi
\hfill Tout nombre premier mesure infalliblement
\hfill une des puissances - 1 de quelque progression que ce soit,
\hfill \& l'exposant de ladite puissance est so\^us-multiple du nombre premier donn\'e - 1.
\hfill Et apr\'es qu'on a trouv\'e la premiere puissance qui satisfait \`a la question,
\hfill toutes celles dont les exposans sont multiples de l'exposant de la premiere
\hfill satisfont de m\'eme \`a la question.
\eightss
\smallskip
\hfill --- PIERRE DE FERMAT, {\eightssi in a letter to Bernard Fr\'enicle de Bessy} (1640)
\endgroup%\obeylines
\bigskip\goodbreak

\advsect Applications of number theory

In this section we present a potpourri of interesting ways number theory is applied to make your life better.

\medskip\boldlabel {\ninepoint\bf ISBN} book identifiers. Every published book has an
{\mc ISBN} code that serves as its
unique identifier. This code is $10$ digits long for books published before 2007, but three new digits
have been added for books published after 2007. In this section we'll deal with the simpler case of $10$ digits.

What we want to do is to bake some redundancy into the codes, so that if a single digit is typed wrong,
a computer will be able to tell the user that the code is invalid. This allows the user to correct their
mistake. The way we do this, in a ten-digit code $d_{10}d_9d_8\cdots d_2d_1$,
is to have only the first nine digits encode the book. The last digit is a {\it check digit},
chosen so that
\edef\eqvalidisbn{\the\eqcount}
$$\sum_{i=1}^{10} i d_i \equiv 0 \pmod{11}.\adveq$$
In other words,
$$d_1\equiv -\sum_{i=2}^{10} id_i \pmod{11}.$$
If $d_1$ needs to equal $10$, we use the symbol `X'.

How does this solve our problem? Well, if a hapless person takes a valid {\mc ISBN} code
and mangles it by either getting one digit wrong, or by swapping two adjacent digits,
the result is an invalid {\mc ISBN} code, and the computer will be able to flag it.
We formalise this statement as the following theorem.

\proclaim Theorem \advthm. Let $d_{10} d_9 d_8\cdots d_2 d_1$ be a valid {\mc ISBN}
code; that is, a code that satisfies~\refeq{\eqvalidisbn}. Then any code obtained by either
\medskip
\item{i)} changing exactly one digit $d_i$, for some $1\le i\le 10$, or
\smallskip
\item{ii)} swapping {\it distinct} adjacent digits $d_i$ and $d_{i-1}$, for some $2\le i\le 10$,
\medskip
is not a valid {\mc ISBN} codes (it does not satisfy~\refeq{\eqvalidisbn}).

\proof First we prove (i). Suppose that $c_{10} c_9 c_8\cdots c_2c_1$ is a code that is
equal to $d_{10} d_9 d_8\cdots d_2 d_1$ except at one place $1\le j\le 10$. So $c_j\ne d_j$,
but for all $1\le i\le 10$ with $i\ne j$, we have $c_i = d_i$. Then
$$\eqalign{
\sum_{i=1}^{10} i c_i &\equiv_11 \sum_{i=1}^{10} i c_i - 0 \cr
&\equiv_11 \sum_{i=1}^{10} i c_i - \sum_{i=1}^{10} id_i \cr
&= \sum_{i=1}^{10} i(c_i - d_i) \cr
&= j(c_j-d_j). \cr
}$$
But $j\in \{1,\ldots, 10\}$ and $(c_j-d_j) \in [-10, 10]$. So neither $j$ nor $c_j-d_j$
are congruent to $0$ modulo $11$. Since $11$ is prime, their product cannot be congruent to $0$
modulo $11$. We conclude that $\sum_{i=1}^{10} i c_i\not\equiv 0\pmod 11$; i.e., this is
not a valid {\mc ISBN} code.

To prove part (ii), suppose that $c_{10} c_9 c_8\cdots c_2c_1$ is a code that is
equal to $d_{10} d_9 d_8\cdots d_2 d_1$, except that for some $2\le j\le 10$, we
have $c_j = d_{j-1}$ and $c_{j-1}=d_j$ (the adjacent digits are swapped). Furthermore,
assume that $d_j\ne d_{j-1}$, since swapping adjacent digits that are the same doesn't
do anything to the code. Then
$$\eqalign{
\sum_{i=1}^{10} i c_i &\equiv_11 \sum_{i=1}^{10} i c_i - 0 \cr
&\equiv_11 \sum_{i=1}^{10} i c_i - \sum_{i=1}^{10} id_i \cr
&= \sum_{i=1}^{10} i(c_i - d_i) \cr
&= (j-1)c_{j-1} + jc_j - (j-1)d_{j-1} - jd_j \cr
&= (j-1)d_j + jd_{j-1} - (j-1)d_{j-1} - jd_j \cr
&= d_{j-1} - d_j.\cr
}$$
But since $d_{j-1}$ and $d_j$ are both in the range $\{0,\ldots, 10\}$, their difference
is in the range $[-10,10]$, and we assumed that $d_{j-1}\ne d_j$, so this difference is not zero.
We conclude that $d_{j-1}-d_j\not\equiv 0\pmod 11$, so the code is not valid.\slug

Something similar is done with credit card numbers, though we don't take digits modulo $11$
(or else we'd have to use the digit 'X', which is cumbersome).
Instead, a slightly different method called Luhn's algorithm is used. The calculation
used to find the check digit is a bit different (let's not get into it),
but the result is that we can still detect
single-digit errors, and we can detect all swapping errors except for the transposition
$09\leftrightarrow 90$.

\medskip\boldlabel Divisibility tests.
In grade school you might have learned the following divisiblity rule:
{\sl To tell if a number is divisible by $3$, you sum up all of its digits. If this is a
multiple of $3$, then the original number was a multiple of $3$, and if not, then the original
number wasn't.} The same holds with $3$ replaced by $9$.

Using the number theory we've learned, we are now able to prove this fact.

\proclaim Proposition \advthm. Suppose that $n\in \NN$ is written in base-$10$ digits as
$$n = d_k d_{k-1} d_{k-2}\cdots d_2 d_1 d_0,$$
for some integer $k\ge 0$ and $d_i\in \{0,\ldots,9\}$ for $0\le i\le k$.
Then $n$ is divisible by $3$ if and only if
$$\sum_{i=0}^k d_i$$
is also divisible by $3$.

\proof Since $10\equiv 1\pmod 3$, any power of $10$ is also congruent to $1$ modulo $3$. So we have
$$n = \sum_{i=0}^k d_i 10^i \equiv_3 \sum_{i=0}^k d_i,$$
and we see that $n$ is divisible by $3$ if and only if the sum on the right-hand side is.\slug

This works with $3$ replaced by $9$ since we also have $10\equiv 1\pmod 9$.

